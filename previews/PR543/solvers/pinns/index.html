<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Physics-Informed Neural Networks · NeuralPDE.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-90474609-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://neuralpde.sciml.ai/stable/solvers/pinns/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralPDE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralPDE.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations</a></li><li><span class="tocitem">Physics-Informed Neural Network Tutorials</span><ul><li><a class="tocitem" href="../../pinn/poisson/">Poisson Equation</a></li><li><a class="tocitem" href="../../pinn/wave/">1D Wave Equation with Dirichlet boundary conditions</a></li><li><a class="tocitem" href="../../pinn/2D/">2-dimensional PDEs with GPU</a></li><li><a class="tocitem" href="../../pinn/system/">Systems of PDEs</a></li><li><a class="tocitem" href="../../pinn/3rd/">ODE with a 3rd-Order Derivative</a></li><li><a class="tocitem" href="../../pinn/low_level/">1-D Burgers&#39; Equation With Low-Level API</a></li><li><a class="tocitem" href="../../pinn/ks/">Kuramoto–Sivashinsky equation</a></li><li><a class="tocitem" href="../../pinn/fp/">Fokker-Planck Equation</a></li><li><a class="tocitem" href="../../pinn/parm_estim/">Optimising Parameters of a Lorenz System</a></li><li><a class="tocitem" href="../../pinn/heterogeneous/">Differential Equations with Heterogeneous Inputs</a></li><li><a class="tocitem" href="../../pinn/integro_diff/">Integro Differential Equations</a></li><li><a class="tocitem" href="../../pinn/debugging/">Debugging PINN Solutions</a></li><li><a class="tocitem" href="../../pinn/neural_adapter/">Transfer Learning with Neural Adapter</a></li></ul></li><li><span class="tocitem">Specialized Neural PDE Tutorials</span><ul><li><a class="tocitem" href="../../examples/kolmogorovbackwards/">Solving Kolmogorov Equations with Neural Networks</a></li><li><a class="tocitem" href="../../examples/optimal_stopping_american/">Optimal Stopping Times of American Options</a></li></ul></li><li><span class="tocitem">Specialized Neural ODE Tutorials</span><ul><li><a class="tocitem" href="../../examples/ode/">Solving ODEs with Physics-Informed Neural Networks</a></li><li><a class="tocitem" href="../../examples/nnrode_example/">Solving Random Ordinary Differential Equations</a></li></ul></li><li><span class="tocitem">API Documentation</span><ul><li><a class="tocitem" href="../ode/">ODE-Specialized Physics-Informed Neural Solver</a></li><li class="is-active"><a class="tocitem" href>Physics-Informed Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Training-strategy"><span>Training strategy</span></a></li><li><a class="tocitem" href="#Transfer-Learning-with-neural_adapter"><span>Transfer Learning with neural_adapter</span></a></li><li><a class="tocitem" href="#Low-level-API"><span>Low-level API</span></a></li></ul></li><li><a class="tocitem" href="../training_strategies/">Training Strategies</a></li><li><a class="tocitem" href="../kolmogorovbackwards_solver/">Neural Network Solvers for Kolmogorov Backwards Equations</a></li><li><a class="tocitem" href="../optimal_stopping/">Neural Network Solvers for Optimal Stopping Time Problems</a></li><li><a class="tocitem" href="../nnrode/">Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Physics-Informed Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Physics-Informed Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/NeuralPDE.jl/blob/master/docs/src/solvers/pinns.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Physics-Informed-Neural-Networks"><a class="docs-heading-anchor" href="#Physics-Informed-Neural-Networks">Physics-Informed Neural Networks</a><a id="Physics-Informed-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Physics-Informed-Neural-Networks" title="Permalink"></a></h1><p>Using the PINNs solver, we can solve general nonlinear PDEs:</p><p><img src="https://user-images.githubusercontent.com/12683885/86625781-5648c800-bfce-11ea-9d99-fbcb5c37fe0c.png" alt="generalPDE"/></p><p>with suitable boundary conditions:</p><p><img src="https://user-images.githubusercontent.com/12683885/86625874-8001ef00-bfce-11ea-9417-1a216c7d90aa.png" alt="bcs"/></p><p>where time t is a special component of x, and Ω contains the temporal domain.</p><p>PDEs are defined using the ModelingToolkit.jl <code>PDESystem</code>:</p><pre><code class="language-julia hljs">@named pde_system = PDESystem(eq,bcs,domains,param,var)</code></pre><p>Here, <code>eq</code> is the equation, <code>bcs</code> represents the boundary conditions, <code>param</code> is the parameter of the equation (like <code>[x,y]</code>), and <code>var</code> represents variables (like <code>[u]</code>).</p><p>The <code>PhysicsInformedNN</code> discretizer is defined as:</p><pre><code class="language-julia hljs">discretization = PhysicsInformedNN(chain,
                                   strategy;
                                   init_params = nothing,
                                   phi = nothing,
                                   derivative = nothing,
                                   )</code></pre><p>Keyword arguments:</p><ul><li><code>chain</code> is a Flux.jl chain, where the input of NN equals the number of dimensions and output equals the number of equations in the system,</li><li><code>strategy</code> determines which training strategy will be used,</li><li><code>init_params</code> is the initial parameter of the neural network. If nothing then automatically generated from the neural network,</li><li><code>phi</code> is a trial solution,</li><li><code>derivative</code> is a method that calculates the derivative.</li></ul><p>The method <code>discretize</code> interprets from the ModelingToolkit PDE form to the PINNs Problem.</p><pre><code class="language-julia hljs">prob = discretize(pde_system, discretization)</code></pre><p>which outputs an <code>OptimizationProblem</code> for <a href="https://Optimization.sciml.ai/dev/">Optimization.jl</a>.</p><h2 id="Training-strategy"><a class="docs-heading-anchor" href="#Training-strategy">Training strategy</a><a id="Training-strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Training-strategy" title="Permalink"></a></h2><p>List of training strategies that are available now:</p><ul><li><p><code>GridTraining(dx)</code>: Initialize points on a lattice uniformly spaced via <code>dx</code>. If <code>dx</code> is a scalar, then <code>dx</code> corresponds to the spacing in each direction. If <code>dx</code> is a vector, then it should be sized to match the number of dimensions and corresponds to the spacing per direction.</p></li><li><p><code>StochasticTraining(points;bcs_points = points)</code>: <code>points</code> is number of stochastically sampled points from the domain,  <code>bcs_points</code> is number of points for boundary conditions(by default, it equals <code>points</code>).    In each optimization iteration, we randomly select a new subset of points from a full training set.</p></li><li><p><code>QuasiRandomTraining(points;bcs_points = points, sampling_alg = UniformSample(), resampling = true, minibatch=500)</code>: The training set is generated on quasi-random low discrepency sequences. <code>points</code> is the number of quasi-random points in every subset or set, <code>bcs_points</code> is number of points for boundary conditions(by default, it equals <code>points</code>), <code>sampling_alg</code> is the quasi-Monte Carlo sampling algorithm. <code>if resampling = false</code>, the full training set is generated in advance before training, and at each iteration, one subset is randomly selected out of the batch.<code>minibatch</code> is the number of subsets in full training set. The number of the total points is <code>length(lb) * points * minibatch</code>, where <code>lb</code> is the lower bound and <code>length(lb)</code> is the dimensionality. <code>if resampling = true</code>, the training set isn&#39;t generated beforehand, and one set of quasi-random points is generated directly at each iteration in runtime. In this case <code>minibatch</code> has no effect.</p><p>See the <a href="https://github.com/SciML/QuasiMonteCarlo.jl">QuasiMonteCarlo.jl</a> for the full set of quasi-random sampling algorithms which are available.</p></li><li><p><code>QuadratureTraining(;quadrature_alg=CubatureJLh(),reltol= 1e-6,abstol= 1e-3,maxiters=1e3,batch=100)</code>:</p></li></ul><p>The loss is computed as an approximation of the integral of the PDE loss   at each iteration using <a href="https://en.wikipedia.org/wiki/Adaptive_quadrature">adaptive quadrature methods</a>   via the differentiable <a href="https://github.com/SciML/Quadrature.jl">Quadrature.jl</a>.</p><ul><li><code>quadrature_alg</code> is quadrature algorithm,</li><li><code>reltol</code>: relative tolerance,</li><li><code>abstol</code>: absolute tolerance,</li><li><code>maxiters</code>: the maximum number of iterations in quadrature algorithm,</li><li><code>batch</code>: the preferred number of points to batch. If <code>batch</code> = 0, the number of points in the batch is determined adaptively by the algorithm.</li></ul><p>See the <a href="https://github.com/SciML/Quadrature.jl">Quadrature.jl</a> documentation for the choices of quadrature methods.</p><h2 id="Transfer-Learning-with-neural_adapter"><a class="docs-heading-anchor" href="#Transfer-Learning-with-neural_adapter">Transfer Learning with neural_adapter</a><a id="Transfer-Learning-with-neural_adapter-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-Learning-with-neural_adapter" title="Permalink"></a></h2><p>Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.</p><ul><li><p><code>neural_adapter(loss_function,initθ,pde_system,strategy)</code>: the method that trains a neural network using the results from one already obtained prediction.   Keyword arguments:</p><ul><li><code>loss_function</code>:the body of loss function,</li><li><code>initθ</code>: the initial parameter of new neural networks,</li><li><code>pde_system</code>: PDE are defined using the ModelingToolkit.jl ,</li><li><code>strategy</code>: determines which training strategy will be used,</li></ul></li><li><p><code>neural_adapter(loss_functions::Array,initθ,pde_systems::Array,strategy)</code>: the method that trains a neural network using the results from many already obtained predictions. Keyword arguments:</p><ul><li><code>loss_functions</code>: the body of loss functions,</li><li><code>initθ</code>: the initial parameter of the neural network,</li><li><code>pde_systems</code>: PDEs are defined using the ModelingToolkit.jl,</li><li><code>strategy</code>: determines which training strategy will be used.</li></ul></li></ul><h2 id="Low-level-API"><a class="docs-heading-anchor" href="#Low-level-API">Low-level API</a><a id="Low-level-API-1"></a><a class="docs-heading-anchor-permalink" href="#Low-level-API" title="Permalink"></a></h2><p>These additional methods exist to help with introspection:</p><ul><li><p><code>symbolic_discretize(pde_system,discretization)</code>: This method is the same as <code>discretize</code> but instead returns the unevaluated Julia function to allow the user to see the generated training code.</p></li><li><p><code>build_symbolic_loss_function(eqs,indvars,depvars, phi, derivative, initθ; bc_indvars=nothing)</code>: return symbolic inner representation for the loss function.   Keyword arguments:</p><ul><li><code>eqs</code>: equation or equations,</li><li><code>indvars</code>: independent variables (the parameter of the equation),</li><li><code>depvars</code>: dependent variables,</li><li><code>phi</code>:trial solution,</li><li><code>derivative</code>: method that calculates the derivative,</li><li><code>initθ</code>: the initial parameter of the neural network,</li><li><code>bc_indvars</code>: independent variables for each boundary conditions.</li></ul></li><li><p><code>build_symbolic_equation(eq,indvars,depvars)</code>: return symbolic inner representation for the equation.</p></li><li><p><code>build_loss_function(eqs, indvars, depvars, phi, derivative, initθ; bc_indvars=nothing)</code>: returns the body of loss function, which is the executable Julia function, for the main equation or boundary condition.</p></li><li><p><code>get_loss_function(loss_functions, train_sets, strategy::TrainingStrategies; τ = nothing)</code>: return the executable loss function.  Keyword arguments:</p><ul><li><code>loss_functions</code>: the body of loss function, which is created using  <code>build_loss_function</code>,</li><li><code>train_sets</code>: training sets,</li><li><code>strategy</code>: training strategy,</li><li><code>τ</code>: normalizing coefficient for loss function. If <code>τ</code> is nothing, then it is automatically set to <code>1/n</code> where <code>n</code> is the number of points checked in the loss function.</li></ul></li><li><p><code>get_phi(chain)</code>: return function for trial solution.</p><ul><li><code>chain</code>: neural network, </li></ul></li><li><p><code>get_numeric_derivative()</code>: return method that calculates the derivative.</p></li><li><p><code>generate_training_sets(domains,dx,bcs,_indvars::Array,_depvars::Array)</code>: return training sets for equations and boundary condition, that is used for GridTraining strategy.</p></li><li><p><code>get_variables(eqs,_indvars::Array,_depvars::Array)</code>: returns all variables that are used in each equations or boundary condition.</p></li><li><p><code>get_argument(eqs,_indvars::Array,_depvars::Array)</code>: returns all arguments that are used in each equations or boundary condition.</p></li><li><p><code>get_bounds(domains,bcs,_indvars::Array,_depvars::Array)</code>: return pairs with lower and upper bounds for all domains. It is used for all non-grid training strategy: StochasticTraining, QuasiRandomTraining, QuadratureTraining.</p></li></ul><p>See how this can be used in <code>Debugging</code> section or <code>2-D Burgers equation, low-level API</code>  examples.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ode/">« ODE-Specialized Physics-Informed Neural Solver</a><a class="docs-footer-nextpage" href="../training_strategies/">Training Strategies »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Sunday 26 June 2022 19:32">Sunday 26 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
