var documenterSearchIndex = {"docs":
[{"location":"examples/blackscholes/#Solving-the-100-dimensional-Black-Scholes-Barenblatt-Equation","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Black Scholes equation is a model for stock option price. In 1973, Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model, which is widely used in financing engineering for computing the option price over time. [1.] In this example, we will solve a Black-Scholes-Barenblatt equation of 100 dimensions. The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation, which models uncertain volatility and interest rates derived from the Black-Scholes equation. This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"To solve it using the TerminalPDEProblem, we write:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"d = 100 # number of dimensions\nX0 = repeat([1.0f0, 0.5f0], div(d,2)) # initial value of stochastic state\ntspan = (0.0f0,1.0f0)\nr = 0.05f0\nsigma = 0.4f0\nf(X,u,σᵀ∇u,p,t) = r * (u - sum(X.*σᵀ∇u))\ng(X) = sum(X.^2)\nμ_f(X,p,t) = zero(X) #Vector d x 1\nσ_f(X,p,t) = Diagonal(sigma*X) #Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d-dimensional -> 1-dimensional chain, while σᵀ∇u needs to be d+1-dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"hls  = 10 + d #hide layer size\nopt = Flux.ADAM(0.001)\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\nσᵀ∇u = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"And now we solve the PDE. Here, we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 150 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-6.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"ans = solve(prob, pdealg, verbose=true, maxiters=150, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-6)","category":"page"},{"location":"examples/blackscholes/#Reference","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Reference","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Shinde, A. S., and K. C. Takale. \"Study of Black-Scholes model and its applications.\" Procedia Engineering 38 (2012): 270-279.","category":"page"},{"location":"examples/nnrode_example/#Solving-Random-Ordinary-Differential-Equations","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"In this tutorial we will solve a RODE with NNRODE. Consider the equation:","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"du = f(uptW)dt","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"where f(uptW)=2usin(W) and W(t) is a Noise process.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"f = (u,p,t,W) ->   2u*sin(W)\ntspan = (0.00f0, 1.00f0)\nu0 = 1.0f0\ndt = 1/20f0","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"We start off by defining the NoiseProcess W(t). In this case, we define a simple Gaussian Process. See Noise Processes for defining other types of processes.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"W = WienerProcess(0.0,0.0,nothing)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"Then, we need to define our model. In order to define a model, we can use Flux.chain or DiffEqFlux.FastChain.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"chain = Flux.Chain(Dense(2,5,elu),Dense(5,1)) #Model using Flux","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"chain = FastChain(FastDense(2,50,tanh), FastDense(50,2)) #Model using DiffEqFlux","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"And let's define our optimizer function:","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"opt = ADAM(1e-3)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"Now, let's pass all the parameters to the algorithm and then call the solver. If we already have some initial parameters, we can pass them into the NNRODE as well.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"alg = NNRODE(chain , W , opt , init_params)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"sol = solve(prob, NeuralPDE.NNRODE(chain,W,opt), dt=dt, verbose = true,\n            abstol=1e-10, maxiters = 15000)","category":"page"},{"location":"pinn/debugging/#Debugging-PINN-Solutions","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"","category":"section"},{"location":"pinn/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"Let's walk through debugging functions for the physics-informed neural network PDE solvers.","category":"page"},{"location":"pinn/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"using NeuralPDE, ModelingToolkit, Flux, DiffEqFlux, Zygote\nimport ModelingToolkit: Interval, infimum, supremum\n# 2d wave equation, neumann boundary condition\n@parameters x, t\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n#2D PDE\nC=1\neq  = Dtt(u(x,t)) ~ C^2*Dxx(u(x,t))\n\n# Initial and boundary conditions\nbcs = [u(0,t) ~ 0.,\n       u(1,t) ~ 0.,\n       u(x,0) ~ x*(1. - x),\n       Dt(u(x,0)) ~ 0. ]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           t ∈ Interval(0.0,1.0)]\n\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\ninitθ = DiffEqFlux.initial_params(chain)\n\neltypeθ = eltype(initθ)\nparameterless_type_θ = DiffEqBase.parameterless_type(initθ)\nphi = NeuralPDE.get_phi(chain,parameterless_type_θ)\nderivative = NeuralPDE.get_numeric_derivative()\n\nu_ = (cord, θ, phi)->sum(phi(cord, θ))\n\nphi([1,2], initθ)\n\nphi_ = (p) -> phi(p, initθ)[1]\ndphi = Zygote.gradient(phi_,[1.,2.])\n\ndphi1 = derivative(phi,u_,[1.,2.],[[ 0.0049215667, 0.0]],1,initθ)\ndphi2 = derivative(phi,u_,[1.,2.],[[0.0,  0.0049215667]],1,initθ)\nisapprox(dphi[1][1], dphi1, atol=1e-8)\nisapprox(dphi[1][2], dphi2, atol=1e-8)\n\n\nindvars = [x,t]\ndepvars = [u(x, t)]\ndim = length(domains)\ndx = 0.1\nstrategy = NeuralPDE.GridTraining(dx)\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq,indvars,depvars,phi,derivative,chain,initθ,strategy)\n\njulia> expr_pde_loss_function = NeuralPDE.build_symbolic_loss_function(eq,indvars,depvars,phi,derivative,chain,initθ,strategy)\n\n:((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667], [0.0, 0.0049215667]], 2, var\"##θ#529\") .- derivative.(phi, u, cord, Array{Float32,1}[[0.0049215667, 0.0], [0.0049215667, 0.0]], 2, var\"##θ#529\")\n              end\n          end\n      end)\n\njulia> bc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n4-element Array{Array{Any,1},1}:\n [:t]\n [:t]\n [:x]\n [:x]\n\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars,\n                                                     phi,derivative,chain,initθ,strategy,\n                                                     bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n\njulia> expr_bc_loss_functions = [NeuralPDE.build_symbolic_loss_function(bc,indvars,depvars,\n                                                                        phi,derivative,chain,initθ,strategy,\n                                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n4-element Array{Expr,1}:\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- (*).(x, (+).(1.0, (*).(-1, x)))\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667]], 1, var\"##θ#529\") .- 0.0\n              end\n          end\n      end)\n\ntrain_sets = NeuralPDE.generate_training_sets(domains,dx,[eq],bcs,eltypeθ,indvars,depvars)\npde_train_set,bcs_train_set = train_sets\n\njulia> pde_train_set\n1-element Array{Array{Float32,2},1}:\n [0.1 0.2 … 0.8 0.9; 0.1 0.1 … 1.0 1.0]\n\n\njulia> bcs_train_set\n4-element Array{Array{Float32,2},1}:\n [0.0 0.0 … 0.0 0.0; 0.0 0.1 … 0.9 1.0]\n [1.0 1.0 … 1.0 1.0; 0.0 0.1 … 0.9 1.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n\n\npde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains,[eq],bcs,eltypeθ,indvars,depvars,NeuralPDE.StochasticTraining(100))\n\njulia> pde_bounds\n1-element Vector{Vector{Any}}:\n [Float32[0.01, 0.99], Float32[0.01, 0.99]]\n\njulia> bcs_bounds\n4-element Vector{Vector{Any}}:\n [0, Float32[0.0, 1.0]]\n [1, Float32[0.0, 1.0]]\n [Float32[0.0, 1.0], 0]\n [Float32[0.0, 1.0], 0]\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,strategy)\n\n@named pde_system = PDESystem(eq,bcs,domains,indvars,depvars)\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nexpr_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\nexpr_pde_loss_function , expr_bc_loss_functions = expr_prob\n","category":"page"},{"location":"pinn/system/#Systems-of-PDEs","page":"Systems of PDEs","title":"Systems of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"In this example, we will solve the PDE system:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginalign*\n_t u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with the initial conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"and the boundary conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with physics-informed neural networks.","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),\n       Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),\n       0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]\n\nbcs = [u1(0,x) ~ sin(pi*x),\n       u2(0,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       u1(t,0) ~ 0.,\n       u2(t,0) ~ exp(-t),\n       u1(t,1) ~ 0.,\n       u2(t,1) ~ -exp(-t)]\n\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain =[FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1)) for _ in 1:3]\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\n_strategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, _strategy, init_params= initθ)\n\n@named pde_system = PDESystem(eqs,bcs,domains,[t,x],[u1(t, x),u2(t, x),u3(t, x)])\nprob = discretize(pde_system,discretization)\nsym_prob = symbolic_discretize(pde_system,discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\nbcs_inner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\n\ncb = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = GalacticOptim.solve(prob,BFGS(); cb = cb, maxiters=5000)\n\nphi = discretization.phi","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"Low-level api","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),\n       Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),\n       0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]\n\nbcs = [u1(0,x) ~ sin(pi*x),\n       u2(0,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       u1(t,0) ~ 0.,\n       u2(t,0) ~ exp(-t),\n       u1(t,1) ~ 0.,\n       u2(t,1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain =[FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1)) for _ in 1:3]\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\nflat_initθ = reduce(vcat,initθ )\n\neltypeθ = eltype(initθ[1])\nparameterless_type_θ = DiffEqBase.parameterless_type(initθ[1])\nphi = NeuralPDE.get_phi.(chain,parameterless_type_θ)\n\nmap(phi_ -> phi_(rand(2,10), flat_initθ),phi)\n\nderivative = NeuralPDE.get_numeric_derivative()\n\n\nindvars = [t,x]\ndepvars = [u1,u2,u3]\ndim = length(domains)\nquadrature_strategy = NeuralPDE.QuadratureTraining()\n\n\n_pde_loss_functions = [NeuralPDE.build_loss_function(eq,indvars,depvars,phi,derivative,\n                                                     chain,initθ,quadrature_strategy) for eq in  eqs]\n\nmap(loss_f -> loss_f(rand(2,10), flat_initθ),_pde_loss_functions)\n\nbc_indvars = NeuralPDE.get_argument(bcs,indvars,depvars)\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars, phi, derivative,\n                                                    chain,initθ,quadrature_strategy,\n                                                    bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\nmap(loss_f -> loss_f(rand(1,10), flat_initθ),_bc_loss_functions)\n\n# dx = 0.1\n# train_sets = NeuralPDE.generate_training_sets(domains,dx,eqs,bcs,eltypeθ,indvars,depvars)\n# pde_train_set,bcs_train_set = train_sets\npde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains,eqs,bcs,eltypeθ,indvars,depvars,quadrature_strategy)\n\nplbs,pubs = pde_bounds\npde_loss_functions = [NeuralPDE.get_loss_function(_loss,\n                                                 lb,ub,\n                                                 eltypeθ, parameterless_type_θ,\n                                                 quadrature_strategy)\n                                                 for (_loss,lb,ub) in zip(_pde_loss_functions, plbs,pubs)]\n\nmap(l->l(flat_initθ) ,pde_loss_functions)\n\nblbs,bubs = bcs_bounds\nbc_loss_functions = [NeuralPDE.get_loss_function(_loss,lb,ub,\n                                                 eltypeθ, parameterless_type_θ,\n                                                 quadrature_strategy)\n                                                 for (_loss,lb,ub) in zip(_bc_loss_functions, blbs,bubs)]\n\nmap(l->l(flat_initθ) ,bc_loss_functions)\n\nloss_functions =  [pde_loss_functions;bc_loss_functions]\n\nfunction loss_function(θ,p)\n    sum(map(l->l(θ) ,loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, GalacticOptim.AutoZygote())\nprob = GalacticOptim.OptimizationProblem(f_, flat_initθ)\n\ncb_ = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde losses: \", map(l -> l(p), loss_functions[1:3]))\n    println(\"bcs losses: \", map(l -> l(p), loss_functions[4:end]))\n    return false\nend\n\nres = GalacticOptim.solve(prob,Optim.BFGS(); cb = cb_, maxiters=5000)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"And some analysis for both low and high level api:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using Plots\n\nts,xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers_ = [res.minimizer[s] for s in sep]\n\nanalytic_sol_func(t,x) = [exp(-t)*sin(pi*x), exp(-t)*cos(pi*x), (1+pi^2)*exp(-t)]\nu_real  = [[analytic_sol_func(t,x)[i] for t in ts for x in xs] for i in 1:3]\nu_predict  = [[phi[i]([t,x],minimizers_[i])[1] for t in ts  for x in xs] for i in 1:3]\ndiff_u = [abs.(u_real[i] .- u_predict[i] ) for i in 1:3]\nfor i in 1:3\n    p1 = plot(ts, xs, u_real[i],linetype=:contourf,title = \"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i],linetype=:contourf,title = \"predict\");\n    p3 = plot(ts, xs, diff_u[i],linetype=:contourf,title = \"error\");\n    plot(p1,p2,p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: sol_uq1)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: sol_uq2)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: sol_uq3)","category":"page"},{"location":"pinn/system/#Derivative-neural-network-approximation","page":"Systems of PDEs","title":"Derivative neural network approximation","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"The accuracy and stability of numerical derivative decreases with each successive order. The accuracy of the entire solution is determined by the worst accuracy of one of the variables, in our case - the highest degree of the derivative. Derivative neural network approximation is such an approach that using lower-order numeric derivatives and estimates higher-order derivatives with a neural network so that allows an increase in the marginal precision for all optimization.","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"Since u3 is only in the first and second equations, that its accuracy during training is determined by the accuracy of the second numerical derivative u3(t,x) ~ (Dtt(u1(t,x)) -Dxx(u1(t,x))) / sin(pi*x).","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"We approximate the derivative of the neural network with another neural network Dt(u1(t,x)) ~ Dtu1(t,x) and train it along with other equations, and thus we avoid using the second numeric derivative Dt(Dtu1(t,x)).","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\nDt = Differential(t)\nDx = Differential(x)\n@variables u1(..), u2(..), u3(..)\n@variables Dxu1(..),Dtu1(..),Dxu2(..),Dtu2(..)\n\neqs_ = [Dt(Dtu1(t,x)) ~ Dx(Dxu1(t,x)) + u3(t,x)*sin(pi*x),\n        Dt(Dtu2(t,x)) ~ Dx(Dxu2(t,x)) + u3(t,x)*cos(pi*x),\n        exp(-t) ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x)]\n\nbcs_ = [u1(0.,x) ~ sin(pi*x),\n       u2(0.,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       #Dtu1(0,x) ~ -sin(pi*x),\n      # Dtu2(0,x) ~ -cos(pi*x),\n       u1(t,0.) ~ 0.,\n       u2(t,0.) ~ exp(-t),\n       u1(t,1.) ~ 0.,\n       u2(t,1.) ~ -exp(-t)]\n\nder_ = [Dt(u1(t,x)) ~ Dtu1(t,x),\n        Dt(u2(t,x)) ~ Dtu2(t,x),\n        Dx(u1(t,x)) ~ Dxu1(t,x),\n        Dx(u2(t,x)) ~ Dxu2(t,x)]\n\nbcs__ = [bcs_;der_]\n\ninput_ = length(domains)\nn = 15\nchain = [FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1)) for _ in 1:7]\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\ngrid_strategy = NeuralPDE.GridTraining(0.07)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,\n                                             grid_strategy,\n                                             init_params= initθ)\n\nvars = [u1(t,x),u2(t,x),u3(t,x),Dxu1(t,x),Dtu1(t,x),Dxu2(t,x),Dtu2(t,x)]\n@named pde_system = PDESystem(eqs_,bcs__,domains,[t,x],vars)\nprob = NeuralPDE.discretize(pde_system,discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\ninner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\nbcs_inner_loss_functions = inner_loss_functions[1:7]\naprox_derivative_loss_functions = inner_loss_functions[9:end]\n\ncb = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = GalacticOptim.solve(prob, ADAM(0.01); cb = cb, maxiters=2000)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,BFGS(); cb = cb, maxiters=10000)\n\nphi = discretization.phi","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"And some analysis:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using Plots\n\nts,xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\ninitθ = discretization.init_params\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers_ = [res.minimizer[s] for s in sep]\n\nu1_real(t,x) = exp(-t)*sin(pi*x)\nu2_real(t,x) = exp(-t)*cos(pi*x)\nu3_real(t,x) = (1+pi^2)*exp(-t)\nDxu1_real(t,x) = pi*exp(-t)*cos(pi*x)\nDtu1_real(t,x) = -exp(-t)*sin(pi*x)\nDxu2_real(t,x) = -pi*exp(-t)*sin(pi*x)\nDtu2_real(t,x) = -exp(-t)*cos(pi*x)\nanalytic_sol_func_all(t,x) = [u1_real(t,x), u2_real(t,x), u3_real(t,x),\n                              Dxu1_real(t,x),Dtu1_real(t,x),Dxu2_real(t,x),Dtu2_real(t,x)]\n\nu_real  = [[analytic_sol_func_all(t,x)[i] for t in ts for x in xs] for i in 1:7]\nu_predict  = [[phi[i]([t,x],minimizers_[i])[1] for t in ts  for x in xs] for i in 1:7]\ndiff_u = [abs.(u_real[i] .- u_predict[i] ) for i in 1:7]\n\ntitles = [\"u1\",\"u2\",\"u3\",\"Dtu1\",\"Dtu2\",\"Dxu1\",\"Dxu2\"]\nfor i in 1:7\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf,title = \"$(titles[i]), analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf,title = \"predict\");\n    p3 = plot(ts, xs, diff_u[i],linetype=:contourf,title = \"error\");\n    plot(p1,p2,p3)\n    savefig(\"3sol_ub$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: aprNN_sol_u1)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: aprNN_sol_u2)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: aprNN_sol_u3)","category":"page"},{"location":"pinn/system/#Comparison-of-the-second-numerical-derivative-and-numerical-neural-network-derivative","page":"Systems of PDEs","title":"Comparison of the second numerical derivative and numerical + neural network derivative","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: DDu1)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: DDu2)","category":"page"},{"location":"pinn/system/#Solving-Matrices-of-PDEs","page":"Systems of PDEs","title":"Solving Matrices of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"Also, in addition to systems, we can use the matrix form of PDEs:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"@parameters x y\n@variables u[1:2,1:2](..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# matrix PDE\neqs  = @. [(Dxx(u_(x,y)) + Dyy(u_(x,y))) for u_ in u] ~ -sin(pi*x)*sin(pi*y)*[0 1; 0 1]\n\n# Initial and boundary conditions\nbcs = [u[1](x,0) ~ x, u[2](x,0) ~ 2, u[3](x,0) ~ 3, u[4](x,0) ~ 4]","category":"page"},{"location":"pinn/system/#Linear-parabolic-system-of-PDEs","page":"Systems of PDEs","title":"Linear parabolic system of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"We can use NeuralPDE to solve the linear parabolic system of PDEs:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nfracpartial upartial t = a * fracpartial^2 upartial x^2 + b_1 u + c_1 w \nfracpartial wpartial t = a * fracpartial^2 wpartial x^2 + b_2 u + c_2 w \nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with initial and boundary conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nu(0 x) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot cos(fracxa) \nw(0 x) = 0 \nu(t 0) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t  w(t 0) = frace^lambda_1-e^lambda_2lambda_1 - lambda_2 \nu(t 1) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t * cos(fracxa) \nw(t 1) = frace^lambda_1 cos(fracxa)-e^lambda_2cos(fracxa)lambda_1 - lambda_2\nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with a physics-informed neural network.","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Plots\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDxx = Differential(x)^2\nDt = Differential(t)\n\n# Constants\na  = 1\nb1 = 4\nb2 = 2\nc1 = 3\nc2 = 1\nλ1 = (b1 + c2 + sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\nλ2 = (b1 + c2 - sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\n\n# Analytic solution\nθ(t, x) = exp(-t) * cos(x / a)\nu_analytic(t, x) = (b1 - λ2) / (b2 * (λ1 - λ2)) * exp(λ1 * t) * θ(t, x) - (b1 - λ1) / (b2 * (λ1 - λ2)) * exp(λ2 * t) * θ(t, x)\nw_analytic(t, x) = 1 / (λ1 - λ2) * (exp(λ1 * t) * θ(t, x) - exp(λ2 * t) * θ(t, x))\n\n# Second-order constant-coefficient linear parabolic system\neqs = [Dt(u(x, t)) ~ a * Dxx(u(x, t)) + b1 * u(x, t) + c1 * w(x, t),\n       Dt(w(x, t)) ~ a * Dxx(w(x, t)) + b2 * u(x, t) + c2 * w(x, t)]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n       w(0, x) ~ w_analytic(0, x),\n       u(t, 0) ~ u_analytic(t, 0),\n       w(t, 0) ~ w_analytic(t, 0),\n       u(t, 1) ~ u_analytic(t, 1),\n       w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           t ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [FastChain(FastDense(input_, n, Flux.σ), FastDense(n, n, Flux.σ), FastDense(n, 1)) for _ in 1:2]\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\n_strategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, _strategy, init_params=initθ)\n\n@named pde_system = PDESystem(eqs, bcs, domains, [t,x], [u(t,x),w(t,x)])\nprob = discretize(pde_system, discretization)\nsym_prob = symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\nbcs_inner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\n\ncb = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=5000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i] + 1:acum[i + 1] for i in 1:length(acum) - 1]\nminimizers_ = [res.minimizer[s] for s in sep]\n\nanalytic_sol_func(t,x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real  = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict  = [[phi[i]([t,x], minimizers_[i])[1] for t in ts  for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(ts, xs, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: linear_parabolic_sol_u1) (Image: linear_parabolic_sol_u2)","category":"page"},{"location":"pinn/system/#Nonlinear-elliptic-system-of-PDEs","page":"Systems of PDEs","title":"Nonlinear elliptic system of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"We can also solve nonlinear systems such as the system of nonlinear elliptic PDEs","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nfracpartial^2upartial x^2 + fracpartial^2upartial y^2 = uf(fracuw) + fracuwh(fracuw) \nfracpartial^2wpartial x^2 + fracpartial^2wpartial y^2 = wg(fracuw) + h(fracuw) \nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"where f, g, h are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nu(0y) = y + 1 \nw(1 y) = cosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nw(x0) = cosh(sqrtf(k)) + sinh(sqrtf(k)) \nw(0y) = k(y + 1) \nu(1 y) = kcosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nu(x0) = kcosh(sqrtf(k)) + sinh(sqrtf(k)) \nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k).","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"This is done using a derivative neural network approximation.","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, DifferentialEquations, Roots\nusing Plots\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, y\nDx = Differential(x)\nDy = Differential(y)\n@variables Dxu(..), Dyu(..), Dxw(..), Dyw(..)\n@variables u(..), w(..)\n\n\n# Arbitrary functions\nf(x) = sin(x)\ng(x) = cos(x)\nh(x) = x\nroot(x) = f(x) - g(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Bisection())                            # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nθ(x, y) = (cosh(sqrt(f(k)) * x) + sinh(sqrt(f(k)) * x)) * (y + 1)   # Analytical solution to Helmholtz equation\nw_analytic(x, y) = θ(x, y) - h(k) / f(k)\nu_analytic(x, y) = k * w_analytic(x, y)\n\n# Nonlinear Steady-State Systems of Two Reaction-Diffusion Equations with 3 arbitrary function f, g, h\neqs_ = [Dx(Dxu(x, y)) + Dy(Dyu(x, y)) ~ u(x, y) * f(u(x, y) / w(x, y)) + u(x, y) / w(x, y) * h(u(x, y) / w(x, y)),\n       Dx(Dxw(x, y)) + Dy(Dyw(x, y)) ~ w(x, y) * g(u(x, y) / w(x, y)) + h(u(x, y) / w(x, y))]\n\n# Boundary conditions\nbcs_ = [u(0, y) ~ u_analytic(0, y),\n       u(1, y) ~ u_analytic(1, y),\n       u(x, 0) ~ u_analytic(x, 0),\n       w(0, y) ~ w_analytic(0, y),\n       w(1, y) ~ w_analytic(1, y),\n       w(x, 0) ~ w_analytic(x, 0)]\n\nder_ = [Dy(u(x, y)) ~ Dyu(x, y),\n       Dy(w(x, y)) ~ Dyw(x, y),\n       Dx(u(x, y)) ~ Dxu(x, y),\n       Dx(w(x, y)) ~ Dxw(x, y)]\n\nbcs__ = [bcs_;der_]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [FastChain(FastDense(input_, n, Flux.σ), FastDense(n, n, Flux.σ), FastDense(n, 1)) for _ in 1:6] # 1:number of @variables\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\n_strategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, _strategy, init_params=initθ)\n\nvars = [u(x,y),w(x,y),Dxu(x,y),Dyu(x,y),Dxw(x,y),Dyw(x,y)]\n@named pde_system = PDESystem(eqs_, bcs__, domains, [x,y], vars)\nprob = NeuralPDE.discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\ninner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\nbcs_inner_loss_functions = inner_loss_functions[1:6]\naprox_derivative_loss_functions = inner_loss_functions[7:end]\n\ncb = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=5000)\n\nphi = discretization.phi\n\n# Analysis\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i] + 1:acum[i + 1] for i in 1:length(acum) - 1]\nminimizers_ = [res.minimizer[s] for s in sep]\n\nanalytic_sol_func(x,y) = [u_analytic(x, y), w_analytic(x, y)]\nu_real  = [[analytic_sol_func(x, y)[i] for x in xs for y in ys] for i in 1:2]\nu_predict  = [[phi[i]([x,y], minimizers_[i])[1] for x in xs for y in ys] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(xs, ys, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(xs, ys, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(xs, ys, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"non_linear_elliptic_sol_u$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: non_linear_elliptic_sol_u1) (Image: non_linear_elliptic_sol_u2)","category":"page"},{"location":"pinn/system/#Nonlinear-hyperbolic-system-of-PDEs","page":"Systems of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"Lastly, we may also solve hyperbolic systems like the following","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nfracpartial^2upartial t^2 = fracax^n fracpartialpartial x(x^n fracpartial upartial x) + u f(fracuw)  \nfracpartial^2wpartial t^2 = fracbx^n fracpartialpartial x(x^n fracpartial upartial x) + w g(fracuw)  \nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"where f and g are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nu(0x) = k * j0(ξ(0 x)) + y0(ξ(0 x)) \nu(t0) = k * j0(ξ(t 0)) + y0(ξ(t 0)) \nu(t1) = k * j0(ξ(t 1)) + y0(ξ(t 1)) \nw(0x) = j0(ξ(0 x)) + y0(ξ(0 x)) \nw(t0) = j0(ξ(t 0)) + y0(ξ(t 0)) \nw(t1) = j0(ξ(t 0)) + y0(ξ(t 0)) \nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k), j0 and y0 are the Bessel functions, and ξ(t, x) is:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"beginaligned\nfracsqrtf(k)sqrtfracax^nsqrtfracax^n(t+1)^2 - (x+1)^2\nendaligned","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"We solve this with Neural:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, Roots\nusing SpecialFunctions\nusing Plots\nusing Quadrature,Cubature\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDx = Differential(x)\nDt = Differential(t)\nDtt = Differential(t)^2\n\n# Constants\na = 16\nb = 16\nn = 0\n\n# Arbitrary functions\nf(x) = x^2\ng(x) = 4 * cos(π * x)\nroot(x) = g(x) - f(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Bisection())                # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nξ(t, x) = sqrt(f(k)) / sqrt(a) * sqrt(a * (t + 1)^2 - (x + 1)^2)\nθ(t, x) = besselj0(ξ(t, x)) + bessely0(ξ(t, x))                     # Analytical solution to Klein-Gordon equation\nw_analytic(t, x) = θ(t, x)\nu_analytic(t, x) = k * θ(t, x)\n\n# Nonlinear system of hyperbolic equations\neqs = [Dtt(u(t, x)) ~ a / (x^n) * Dx(x^n * Dx(u(t, x))) + u(t, x) * f(u(t, x) / w(t, x)),\n       Dtt(w(t, x)) ~ b / (x^n) * Dx(x^n * Dx(w(t, x))) + w(t, x) * g(u(t, x) / w(t, x))]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n       w(0, x) ~ w_analytic(0, x),\n       u(t, 0) ~ u_analytic(t, 0),\n       w(t, 0) ~ w_analytic(t, 0),\n       u(t, 1) ~ u_analytic(t, 1),\n       w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [FastChain(FastDense(input_, n, Flux.σ), FastDense(n, n, Flux.σ), FastDense(n, 1)) for _ in 1:2]\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\n_strategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, _strategy, init_params=initθ)\n\n@named pde_system = PDESystem(eqs, bcs, domains, [t,x], [u(t,x),w(t,x)])\nprob = discretize(pde_system, discretization)\nsym_prob = symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\nbcs_inner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\n\ncb = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=1000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i] + 1:acum[i + 1] for i in 1:length(acum) - 1]\nminimizers_ = [res.minimizer[s] for s in sep]\n\nanalytic_sol_func(t,x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real  = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict  = [[phi[i]([t,x], minimizers_[i])[1] for t in ts  for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(ts, xs, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"nonlinear_hyperbolic_sol_u$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: nonlinear_hyperbolic_sol_u1) (Image: nonlinear_hyperbolic_sol_u2)","category":"page"},{"location":"solvers/nnrode/#Random-Ordinary-Differential-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"solvers/nnrode/","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"TODO","category":"page"},{"location":"pinn/wave/#D-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Let's solve this 1-dimensional wave equation:","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginalign*\n^2_t u(x t) = c^2 ^2_x u(x t) quad  textsffor all  0  x  1 text and  t  0   \nu(0 t) = u(1 t) = 0 quad  textsffor all  t  0   \nu(x 0) = x (1-x)     quad  textsffor all  0  x  1   \n_t u(x 0) = 0       quad  textsffor all  0  x  1   \nendalign*","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.1 and physics-informed neural networks.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Further, the solution of this equation with the given boundary conditions is presented.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC=1\neq  = Dtt(u(t,x)) ~ C^2*Dxx(u(t,x))\n\n# Initial and boundary conditions\nbcs = [u(t,0) ~ 0.,# for all t > 0\n       u(t,1) ~ 0.,# for all t > 0\n       u(0,x) ~ x*(1. - x), #for all 0 < x < 1\n       Dt(u(0,x)) ~ 0. ] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n# Discretization\ndx = 0.1\n\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\ninitθ = Float64.(DiffEqFlux.initial_params(chain))\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx); init_params = initθ)\n\n@named pde_system = PDESystem(eq,bcs,domains,[t,x],[u(t,x)])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# optimizer\nopt = Optim.BFGS()\nres = GalacticOptim.solve(prob,opt; cb = cb, maxiters=1200)\nphi = discretization.phi","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using Plots\n\nts,xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nanalytic_sol_func(t,x) =  sum([(8/(k^3*pi^3)) * sin(k*pi*x)*cos(C*k*pi*t) for k in 1:2:50000])\n\nu_predict = reshape([first(phi([t,x],res.minimizer)) for t in ts for x in xs],(length(ts),length(xs)))\nu_real = reshape([analytic_sol_func(t,x) for t in ts for x in xs], (length(ts),length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype=:contourf,title = \"analytic\");\np2 =plot(ts, xs, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(ts, xs, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: waveplot)","category":"page"},{"location":"pinn/wave/#D-Damped-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Damped Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Now let's solve the 1-dimensional wave equation with damping.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginaligned\nfracpartial^2 u(tx)partial x^2 = frac1c^2 fracpartial^2 u(tx)partial t^2 + v fracpartial u(tx)partial t \nu(t 0) = u(t L) = 0 \nu(0 x) = x(1-x) \nu_t(0 x) = 1 - 2x \nendaligned","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.05 and physics-informed neural networks. Here we take advantage of adaptive derivative to increase accuracy.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Plots, Printf\nusing Quadrature,Cubature, Cuba\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..) Dxu(..) Dtu(..) O1(..) O2(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n# Constants\nv = 3\nb = 2\nL = 1.0\n@assert b > 0 && b < 2π / (L * v)\n\n# 1D damped wave\neq = Dx(Dxu(t, x)) ~ 1 / v^2 * Dt(Dtu(t, x)) + b * Dtu(t, x)\n\n# Initial and boundary conditions\nbcs_ = [u(t, 0) ~ 0.,# for all t > 0\n       u(t, L) ~ 0.,# for all t > 0\n       u(0, x) ~ x * (1. - x), # for all 0 < x < 1\n       Dtu(0, x) ~ 1 - 2x # for all  0 < x < 1\n       ]\n\nep = (cbrt(eps(eltype(Float64))))^2 / 6\n\nder = [Dxu(t, x) ~ Dx(u(t, x)) + ep * O1(t, x),\n       Dtu(t, x) ~  Dt(u(t, x)) + ep * O2(t, x)]\n\nbcs = [bcs_;der]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, L),\n           x ∈ Interval(0.0, L)]\n\n# Neural network\nchain = [[FastChain(FastDense(2, 16, Flux.tanh), FastDense(16, 16, Flux.tanh), FastDense(16, 16, Flux.tanh), FastDense(16, 1)) for _ in 1:3];\n         [FastChain(FastDense(2, 6, Flux.tanh), FastDense(6, 1)) for _ in 1:2];]\n\ninitθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))\n\ndx = 0.05\nstrategy = GridTraining(dx)\ndiscretization = PhysicsInformedNN(chain, strategy; init_params=initθ)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x), Dxu(t, x), Dtu(t, x), O1(t, x), O2(t, x)])\nprob = discretize(pde_system, discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\ninner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\nbcs_inner_loss_functions = inner_loss_functions\n\ncb = function (p, l)\n    println(\"Current loss is: $l\")\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\n# Optimizer\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=1000)\nprob = remake(prob, u0=res.minimizer)\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=3000)\n\nphi = discretization.phi[1]\n\n# Analysis\nts, xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\n\nμ_n(k) = (v * sqrt(4 * k^2 * π^2 - b^2 * L^2 * v^2)) / (2 * L)\nb_n(k) = 2 / L * -(L^2 * ((2 * π * L - π) * k * sin(π * k) + ((π^2 - π^2 * L) * k^2 + 2 * L) * cos(π * k) - 2 * L)) / (π^3 * k^3) # vegas((x, ϕ) -> ϕ[1] = sin(k * π * x[1]) * f(x[1])).integral[1]\na_n(k) = 2 / -(L * μ_n(k)) * (L * (((2 * π * L^2 - π * L) * b * k * sin(π * k) + ((π^2 * L - π^2 * L^2) * b * k^2 + 2 * L^2 * b) * cos(π * k) - 2 * L^2 * b) * v^2 + 4 * π * L * k * sin(π * k) + (2 * π^2 - 4 * π^2 * L) * k^2 * cos(π * k) - 2 * π^2 * k^2)) / (2 * π^3 * k^3)\n\n# Plot\nanalytic_sol_func(t,x) = sum([sin((k * π * x) / L) * exp(-v^2 * b * t / 2) * (a_n(k) * sin(μ_n(k) * t) + b_n(k) * cos(μ_n(k) * t)) for k in 1:2:100]) # TODO replace 10 with 500\nanim = @animate for t ∈ ts\n    @info \"Time $t...\"\n    sol =  [analytic_sol_func(t, x) for x in xs]\n    sol_p =  [first(phi([t,x], res.minimizer)) for x in xs]\n    plot(sol, label=\"analytic\", ylims=[0, 0.1])\n    title = @sprintf(\"t = %.3f\", t)\n    plot!(sol_p, label=\"predict\", ylims=[0, 0.1], title=title)\nend\ngif(anim, \"1Dwave_damped_adaptive.gif\", fps=200)\n\n# Surface plot\nu_predict = reshape([first(phi([t,x], res.minimizer)) for t in ts for x in xs], (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs], (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype=:contourf, title=\"analytic\");\np2 = plot(ts, xs, u_predict, linetype=:contourf, title=\"predict\");\np3 = plot(ts, xs, diff_u, linetype=:contourf, title=\"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can see the results here:","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: Damped_wave_sol_adaptive_u)","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Plotted as a line one can see the analytical solution and the prediction here:","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: 1Dwave_damped_adaptive)","category":"page"},{"location":"examples/100_HJB/#Solving-a-100-dimensional-Hamilton-Jacobi-Bellman-Equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"First, here's a fully working code for the solution of a 100-dimensional Hamilton-Jacobi-Bellman equation that takes a few minutes on a laptop:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"using NeuralPDE\nusing Flux\nusing DifferentialEquations\nusing LinearAlgebra\nd = 100 # number of dimensions\nX0 = fill(0.0f0, d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nλ = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0 * sum(X.^2))\nf(X,u,σᵀ∇u,p,t) = -λ * sum(σᵀ∇u.^2)\nμ_f(X,p,t) = zero(X)  # Vector d x 1 λ\nσ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)\nhls = 10 + d # hidden layer size\nopt = Flux.ADAM(0.01)  # optimizer\n# sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d, hls, relu),\n                Dense(hls, hls, relu),\n                Dense(hls, 1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)\n@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=1.2, pabstol=1f-2)","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Now, let's explain the details!","category":"page"},{"location":"examples/100_HJB/#H-J-B-equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"H-J-B equation","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"The Hamilton-Jacobi-Bellman equation is the solution to a stochastic optimal control problem.","category":"page"},{"location":"examples/100_HJB/#Symbolic-Solution","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Symbolic Solution","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Here, we choose to solve the classical Linear Quadratic Gaussian (LQG) control problem of 100 dimensions, which is governed by the SDE dX_t = 2sqrt(λ)c_t dt + sqrt(2)dW_t where c_t is a control process. The solution to the optimal control is given by a PDE of the form:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"(Image: HJB)","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"with terminating condition g(X) = log(0.5f0 + 0.5f0*sum(X.^2)).","category":"page"},{"location":"examples/100_HJB/#Solving-LQG-Problem-with-Neural-Net","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving LQG Problem with Neural Net","text":"","category":"section"},{"location":"examples/100_HJB/#Define-the-Problem","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Define the Problem","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"To get the solution above using the TerminalPDEProblem, we write:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"d = 100 # number of dimensions\nX0 = fill(0.0f0,d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nλ = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0*sum(X.^2))\nf(X,u,σᵀ∇u,p,t) = -λ*sum(σᵀ∇u.^2)\nμ_f(X,p,t) = zero(X)  #Vector d x 1 λ\nσ_f(X,p,t) = Diagonal(sqrt(2.0f0)*ones(Float32,d)) #Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)","category":"page"},{"location":"examples/100_HJB/#Define-the-Solver-Algorithm","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Define the Solver Algorithm","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d dimensional -> 1 dimensional chain, while σᵀ∇u needs to be d+1 dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"hls = 10 + d #hidden layer size\nopt = Flux.ADAM(0.01)  #optimizer\n#sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)","category":"page"},{"location":"examples/100_HJB/#Solving-with-Neural-Net","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving with Neural Net","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-2)\n","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Here we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 100 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-2.","category":"page"},{"location":"pinn/ks/#Kuramoto–Sivashinsky-equation","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"Let's consider the Kuramoto–Sivashinsky equation, which contains a 4th-order derivative:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"_t u(x t) + u(x t) _x u(x t) + alpha ^2_x u(x t) + beta ^3_x u(x t) + gamma ^4_x u(x t) =  0  ","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\alpha = \\gamma = 1 and \\beta = 4. The exact solution is:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"u_e(x t) = 11 + 15 tanh theta - 15 tanh^2 theta - 15 tanh^3 theta  ","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\theta = 1 - x/2 and with initial and boundary conditions:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"beginalign*\n    u(  x 0) =     u_e(  x 0)  \n    u( 10 t) =     u_e( 10 t)  \n    u(-10 t) =     u_e(-10 t)  \n_x u( 10 t) = _x u_e( 10 t)  \n_x u(-10 t) = _x u_e(-10 t)  \nendalign*","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"We use physics-informed neural networks.","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, t\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\nα = 1\nβ = 4\nγ = 1\neq = Dt(u(x,t)) + u(x,t)*Dx(u(x,t)) + α*Dx2(u(x,t)) + β*Dx3(u(x,t)) + γ*Dx4(u(x,t)) ~ 0\n\nu_analytic(x,t;z = -x/2+t) = 11 + 15*tanh(z) -15*tanh(z)^2 - 15*tanh(z)^3\ndu(x,t;z = -x/2+t) = 15/2*(tanh(z) + 1)*(3*tanh(z) - 1)*sech(z)^2\n\nbcs = [u(x,0) ~ u_analytic(x,0),\n       u(-10,t) ~ u_analytic(-10,t),\n       u(10,t) ~ u_analytic(10,t),\n       Dx(u(-10,t)) ~ du(-10,t),\n       Dx(u(10,t)) ~ du(10,t)]\n\n# Space and time domains\ndomains = [x ∈ Interval(-10.0,10.0),\n           t ∈ Interval(0.0,1.0)]\n# Discretization\ndx = 0.4; dt = 0.2\n\n# Neural network\nchain = FastChain(FastDense(2,12,Flux.σ),FastDense(12,12,Flux.σ),FastDense(12,1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining([dx,dt]))\n@named pde_system = PDESystem(eq,bcs,domains,[x,t],[u(x, t)])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nopt = Optim.BFGS()\nres = GalacticOptim.solve(prob,opt; cb = cb, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"And some analysis:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using Plots\n\nxs,ts = [infimum(d.domain):dx:supremum(d.domain) for (d,dx) in zip(domains,[dx/10,dt])]\n\nu_predict = [[first(phi([x,t],res.minimizer)) for x in xs] for t in ts]\nu_real = [[u_analytic(x,t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x,t) -first(phi([x,t],res.minimizer)))  for x in xs] for t in ts]\n\np1 =plot(xs,u_predict,title = \"predict\")\np2 =plot(xs,u_real,title = \"analytic\")\np3 =plot(xs,diff_u,title = \"error\")\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: plotks)","category":"page"},{"location":"pinn/parm_estim/#Optimising-Parameters-of-a-Lorenz-System","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"","category":"section"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Consider a Lorenz System ,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"beginalign*\n    fracmathrmd xmathrmdt = sigma (y -x)  \n    fracmathrmd ymathrmdt = x (rho - z) - y  \n    fracmathrmd zmathrmdt = x y - beta z  \nendalign*","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimise the parameters \\sigma, \\beta, and \\rho.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"We start by defining the the problem,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, OrdinaryDiffEq, Plots\nimport ModelingToolkit: Interval, infimum, supremum\n@parameters t ,σ_ ,β, ρ\n@variables x(..), y(..), z(..)\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ_*(y(t) - x(t)),\n       Dt(y(t)) ~ x(t)*(ρ - z(t)) - y(t),\n       Dt(z(t)) ~ x(t)*y(t) - β*z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0,1.0)]\ndt = 0.01","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"And the neural networks as,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"input_ = length(domains)\nn = 8\nchain1 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain2 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain3 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"We will add an additional loss term based on the data that we have in order to optimise the parameters.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Here we simply calculate the solution of the lorenz system with OrdinaryDiffEq.jl based on the adaptivity of the ODE solver. This is used to introduce non-uniformity to the time series.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"function lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,1.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob, Tsit5(), dt=0.1)\nts = [infimum(d.domain):dt:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us,ts_]\nend\ndata = getData(sol)\ninitθs = DiffEqFlux.initial_params.([chain1,chain2,chain3])\nacum =  [0;accumulate(+, length.(initθs))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\n(u_ , t_) = data\nlen = length(data[2])","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Then we define the additional loss funciton additional_loss(phi, θ , p), the function has three arguments, phi the trial solution, θ the parameters of neural networks, and the hyperparameters p .","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"function additional_loss(phi, θ , p)\n    return sum(sum(abs2, phi[i](t_ , θ[sep[i]]) .- u_[[i], :])/len for i in 1:1:3)\nend","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Then finally defining and optimising using the PhysicsInformedNN interface.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"discretization = NeuralPDE.PhysicsInformedNN([chain1 , chain2, chain3],NeuralPDE.GridTraining(dt), param_estim=true, additional_loss=additional_loss)\n@named pde_system = PDESystem(eqs,bcs,domains,[t],[x(t), y(t), z(t)],[σ_, ρ, β], defaults=Dict([p .=> 1.0 for p in [σ_, ρ, β]]))\nprob = NeuralPDE.discretize(pde_system,discretization)\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = GalacticOptim.solve(prob, BFGS(); cb = cb, maxiters=5000)\np_ = res.minimizer[end-2:end] # p_ = [9.93, 28.002, 2.667]","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"And then finally some analyisis by plotting.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"initθ = discretization.init_params\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers = [res.minimizer[s] for s in sep]\nts = [infimum(d.domain):dt/10:supremum(d.domain) for d in domains][1]\nu_predict  = [[discretization.phi[i]([t],minimizers[i])[1] for t in ts] for i in 1:3]\nplot(sol)\nplot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"(Image: Plot_Lorenz)","category":"page"},{"location":"solvers/optimal_stopping/#Neural-Network-Solvers-for-Optimal-Stopping-Time-Problems","page":"Neural Network Solvers for Optimal Stopping Time Problems","title":"Neural Network Solvers for Optimal Stopping Time Problems","text":"","category":"section"},{"location":"solvers/optimal_stopping/","page":"Neural Network Solvers for Optimal Stopping Time Problems","title":"Neural Network Solvers for Optimal Stopping Time Problems","text":"TODO","category":"page"},{"location":"pinn/fp/#Fokker-Planck-Equation","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"","category":"section"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"Let's consider the Fokker-Planck equation:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"- fracx left  left( alpha x - beta x^3right) p(x)right  + fracsigma^22 frac^2x^2 p(x) = 0  ","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"which must satisfy the normalization condition:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"Delta t  p(x) = 1","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"with the boundary conditions:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"p(-22) = p(22) = 0","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"with Physics-Informed Neural Networks.","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n# the example is taken from this article https://arxiv.org/abs/1910.10503\n@parameters x\n@variables p(..)\nDx = Differential(x)\nDxx = Differential(x)^2\n\nα = 0.3\nβ = 0.5\n_σ = 0.5\nx_0 = -2.2\nx_end = 2.2\n# Discretization\ndx = 0.01\n\neq  = Dx((α*x - β*x^3)*p(x)) ~ (_σ^2/2)*Dxx(p(x))\n\n# Initial and boundary conditions\nbcs = [p(x_0) ~ 0. ,p(x_end) ~ 0.]\n\n# Space and time domains\ndomains = [x ∈ Interval(x_0,x_end)]\n\n# Neural network\ninn = 18\nchain = FastChain(FastDense(1,inn,Flux.σ),\n                  FastDense(inn,inn,Flux.σ),\n                  FastDense(inn,inn,Flux.σ),\n                  FastDense(inn,1))\ninitθ = Float64.(DiffEqFlux.initial_params(chain))\n\nlb = [x_0]\nub = [x_end]\nfunction norm_loss_function(phi,θ,p)\n    function inner_f(x,θ)\n         dx*phi(x, θ) .- 1\n    end\n    prob = QuadratureProblem(inner_f, lb, ub, θ)\n    norm2 = solve(prob, HCubatureJL(), reltol = 1e-8, abstol = 1e-8, maxiters =10);\n    abs(norm2[1])\nend\n\ndiscretization = PhysicsInformedNN(chain,\n                                   GridTraining(dx);\n                                   init_params = initθ,\n                                   additional_loss=norm_loss_function)\n\n@named pde_system = PDESystem(eq,bcs,domains,[x],[p(x)])\nprob = discretize(pde_system,discretization)\n\npde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents\nbcs_inner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents\n\nphi = discretization.phi\n\ncb_ = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"additional_loss: \", norm_loss_function(phi,p,nothing))\n    return false\nend\n\nres = GalacticOptim.solve(prob,LBFGS(),cb = cb_,maxiters=400)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,BFGS(),cb = cb_,maxiters=2000)","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"And some analysis:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"using Plots\nC = 142.88418699042 #fitting param\nanalytic_sol_func(x) = C*exp((1/(2*_σ^2))*(2*α*x^2 - β*x^4))\n\nxs = [infimum(d.domain):dx:supremum(d.domain) for d in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.minimizer)) for x in xs]\n\nplot(xs ,u_real, label = \"analytic\")\nplot!(xs ,u_predict, label = \"predict\")","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"(Image: fp)","category":"page"},{"location":"pinn/low_level/#D-Burgers'-Equation-With-Low-Level-API","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"","category":"section"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"Let's consider the Burgers' equation:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"begingather*\n_t u + u _x u - (001  pi) _x^2 u = 0   quad x in -1 1 t in 0 1   \nu(0 x) = - sin(pi x)   \nu(t -1) = u(t 1) = 0  \nendgather*","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"with Physics-Informed Neural Networks. Here is an example of using the low-level API:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n#2D PDE\neq  = Dt(u(t,x)) + u(t,x)*Dx(u(t,x)) - (0.01/pi)*Dxx(u(t,x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [u(0,x) ~ -sin(pi*x),\n       u(t,-1) ~ 0.,\n       u(t,1) ~ 0.,\n       u(t,-1) ~ u(t,1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(-1.0,1.0)]\n# Discretization\ndx = 0.05\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\ninitθ = Float64.(DiffEqFlux.initial_params(chain))\neltypeθ = eltype(initθ)\nparameterless_type_θ = DiffEqBase.parameterless_type(initθ)\nstrategy = NeuralPDE.GridTraining(dx)\n\nphi = NeuralPDE.get_phi(chain,parameterless_type_θ)\nderivative = NeuralPDE.get_numeric_derivative()\n\n\nindvars = [t,x]\ndepvars = [u]\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq,indvars,depvars,\n                                                   phi,derivative,chain,initθ,strategy)\n\nbc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars,\n                                                    phi,derivative,chain,initθ,strategy,\n                                                    bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n\ntrain_sets = NeuralPDE.generate_training_sets(domains,dx,[eq],bcs,eltypeθ,indvars,depvars)\ntrain_domain_set, train_bound_set = train_sets\n\n\npde_loss_function = NeuralPDE.get_loss_function(_pde_loss_function,\n                                                train_domain_set[1],\n                                                eltypeθ,parameterless_type_θ,\n                                                strategy)\n\nbc_loss_functions = [NeuralPDE.get_loss_function(loss,set,\n                                                 eltypeθ, parameterless_type_θ,\n                                                 strategy) for (loss, set) in zip(_bc_loss_functions,train_bound_set)]\n\n\nloss_functions = [pde_loss_function; bc_loss_functions]\nloss_function__ = θ -> sum(map(l->l(θ) ,loss_functions))\n\nfunction loss_function_(θ,p)\n    return loss_function__(θ)\nend\n\nf = OptimizationFunction(loss_function_, GalacticOptim.AutoZygote())\nprob = GalacticOptim.OptimizationProblem(f, initθ)\n\ncb_ = function (p,l)\n    println(\"loss: \", l , \"losses: \", map(l -> l(p), loss_functions))\n    return false\nend\n\n# optimizer\nopt = BFGS()\nres = GalacticOptim.solve(prob, opt; cb = cb_, maxiters=2000)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"And some analysis:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"using Plots\n\nts,xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_contourf = reshape([first(phi([t,x],res.minimizer)) for t in ts for x in xs] ,length(xs),length(ts))\nplot(ts, xs, u_predict_contourf, linetype=:contourf,title = \"predict\")\n\nu_predict = [[first(phi([t,x],res.minimizer)) for x in xs] for t in ts ]\np1= plot(xs, u_predict[3],title = \"t = 0.1\");\np2= plot(xs, u_predict[11],title = \"t = 0.5\");\np3= plot(xs, u_predict[end],title = \"t = 1\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"(Image: burgers)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"(Image: burgers2)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"See low-level API","category":"page"},{"location":"solvers/deep_fbsde/#Deep-Forward-Backwards-SDEs-for-Terminal-Parabolic-PDEs","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"","category":"section"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"To solve high-dimensional PDEs, one should first describe the PDE in terms of the TerminalPDEProblem with constructor:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"TerminalPDEProblem(g,f,μ_f,σ_f,X0,tspan,p=nothing)","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"which describes the semilinear parabolic PDE of the form:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"(Image: paraPDE)","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"with terminating condition u(tspan[2],x) = g(x). These methods solve the PDE in reverse, satisfying the terminal equation and giving a point estimate at u(tspan[1],X0). The dimensionality of the PDE is determined by the choice of X0, which is the initial stochastic state.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"To solve this PDE problem, there exist two algorithms:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"NNPDENS(u0,σᵀ∇u;opt=Flux.ADAM(0.1)): Uses a neural stochastic differential equation, which is then solved by the methods available in DifferentialEquations.jl. The alg keyword is required for specifying the SDE solver algorithm that will be used on the internal SDE. All of the other keyword arguments are passed to the SDE solver.\nNNPDEHan(u0,σᵀ∇u;opt=Flux.ADAM(0.1)): Uses the stochastic RNN algorithm from Han. Only applicable when μ_f and σ_f result in a non-stiff SDE where low order non-adaptive time stepping is applicable.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"Here, u0 is a Flux.jl chain with a d-dimensional input and a 1-dimensional output. For NNPDEHan, σᵀ∇u is an array of M chains with a d-dimensional input and a d-dimensional output, where M is the total number of timesteps. For NNPDENS it is a d+1-dimensional input (where the final value is time) and a d-dimensional output. opt is a Flux.jl optimizer.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"Each of these methods has a special keyword argument pabstol, which specifies an absolute tolerance on the PDE's solution, and will exit early if the loss reaches this value. Its default value is 1f-6.","category":"page"},{"location":"solvers/ode/#ODE-Specialized-Physics-Informed-Neural-Solver","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"The ODE-specialized physics-informed neural network (PINN) solver is a method for the DifferentialEquations.jl common interface of ODEProblem, which generates the solution via a neural network. Thus the standard ODEProblem is used, but a new algorithm, NNODE, is used to solve the problem.","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"The algorithm type is:","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"nnode(chain,opt)","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"where chain is a DiffEqFlux sciml_train-compatible Chain or FastChain representing a neural network, and opt is an optimization method for sciml_train. For more details, see the DiffEqFlux documentation on sciml_train.","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"Lagaris, Isaac E., Aristidis Likas, and Dimitrios I. Fotiadis. \"Artificial neural networks for solving ordinary and partial differential equations.\" IEEE Transactions on Neural Networks 9, no. 5 (1998): 987-1000.","category":"page"},{"location":"pinn/integro_diff/#Integro-Differential-Equations","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"","category":"section"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"The integral of function u(x),","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"int_0^tu(x)dx","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"where x is variable of integral and t is variable of integro differential equation,","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"is defined as","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"using ModelingToolkit\n@parameters t\n@variables i(..)\nIi = Symbolics.Integral(t in DomainSets.ClosedInterval(0, t))","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"In multidimensional case,","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"Ix = Integral((x,y) in DomainSets.UnitSquare())","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"The UnitSquare domain ranges both x and y from 0 to 1. Similarly a rectangular or cuboidal domain can be defined using ProductDomain of ClosedIntervals.","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"Ix = Integral((x,y) in DomainSets.ProductDomain(ClosedInterval(0 ,1), ClosedInterval(0 ,x)))","category":"page"},{"location":"pinn/integro_diff/#dimensional-example","page":"Integro Differential Equations","title":"1-dimensional example","text":"","category":"section"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"Lets take an example of an integro differential equation:","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"fracx u(x)  + 2u(x) + 5 int_0^xu(t)dt = 1 for x geq 0","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"and boundary condition","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"u(0) = 0","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, DomainSets\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t\n@variables i(..)\nDi = Differential(t)\nIi = Integral(t in DomainSets.ClosedInterval(0, t))\neq = Di(i(t)) + 2*i(t) + 5*Ii(i(t)) ~ 1\nbcs = [i(0.) ~ 0.0]\ndomains = [t ∈ Interval(0.0,2.0)]\nchain = Chain(Dense(1,15,Flux.σ),Dense(15,1))\ninitθ = Float64.(DiffEqFlux.initial_params(chain))\n\nstrategy_ = GridTraining(0.05)\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy_;\n                                   init_params = nothing,\n                                   phi = nothing,\n                                   derivative = nothing)\npde_system = PDESystem(eq,bcs,domains,[t],[i(t)])\nprob = NeuralPDE.discretize(pde_system,discretization)\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = GalacticOptim.solve(prob, BFGS(); cb = cb, maxiters=100)","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"Plotting the final solution and analytical solution","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nphi = discretization.phi\nu_predict  = [first(phi([t],res.minimizer)) for t in ts]\n\nanalytic_sol_func(t) = 1/2*(exp(-t))*(sin(2*t))\nu_real  = [analytic_sol_func(t) for t in ts]\nusing Plots\nplot(ts ,u_real, label = \"Analytical Solution\")\nplot!(ts, u_predict, label = \"PINN Solution\")","category":"page"},{"location":"pinn/integro_diff/","page":"Integro Differential Equations","title":"Integro Differential Equations","text":"(Image: IDE)","category":"page"},{"location":"pinn/neural_adapter/#Transfer-Learning-with-Neural-Adapter","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"","category":"section"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"neural_adapter is method that trains a neural network using the results from an already obtained prediction.","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"This allows reusing the obtained prediction results and pre-training states of the neural network to get a new prediction, or reuse the results of predictions to train a related task (for example, the same task with a different domain). It makes it possible to create more flexible training schemes.","category":"page"},{"location":"pinn/neural_adapter/#Retrain-the-prediction","page":"Transfer Learning with Neural Adapter","title":"Retrain the prediction","text":"","category":"section"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Using the example of 2D Poisson equation, it is shown how, using method neural_adapter, to retrain the prediction of one neural network to another.","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: image)","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Initial and boundary conditions\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]\nquadrature_strategy = NeuralPDE.QuadratureTraining(reltol=1e-2,abstol=1e-2,\n                                                   maxiters =50, batch=100)\ninner = 8\naf = Flux.tanh\nchain1 = Chain(Dense(2,inner,af),\n               Dense(inner,inner,af),\n               Dense(inner,1))\n\ninitθ = Float64.(DiffEqFlux.initial_params(chain1))\ndiscretization = NeuralPDE.PhysicsInformedNN(chain1,\n                                             quadrature_strategy;\n                                             init_params = initθ)\n\n@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = NeuralPDE.discretize(pde_system,discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\n\nres = GalacticOptim.solve(prob, BFGS();  maxiters=2000)\nphi = discretization.phi\n\ninner_ = 12\naf = Flux.tanh\nchain2 = FastChain(FastDense(2,inner_,af),\n                   FastDense(inner_,inner_,af),\n                   FastDense(inner_,inner_,af),\n                   FastDense(inner_,1))\n\ninitθ2 =Float64.(DiffEqFlux.initial_params(chain2))\n\n# the rule by which the training will take place is described here in loss function\nfunction loss(cord,θ)\n    chain2(cord,θ) .- phi(cord,res.minimizer)\nend\n\nstrategy = NeuralPDE.GridTraining(0.02)\n\nprob_ = NeuralPDE.neural_adapter(loss, initθ2, pde_system, strategy)\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres_ = GalacticOptim.solve(prob_, BFGS();cb=cb, maxiters=1000)\n\nparameterless_type_θ = DiffEqBase.parameterless_type(initθ2)\nphi_ = NeuralPDE.get_phi(chain2,parameterless_type_θ)\n\nxs,ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_predict_ =  reshape([first(phi_([x,y],res_.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = u_predict .- u_real\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\np1 = plot(xs, ys, u_predict, linetype=:contourf,title = \"first predict\");\np2 = plot(xs, ys, u_predict_,linetype=:contourf,title = \"second predict\");\np3 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np4 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error 1\");\np5 = plot(xs, ys, diff_u_,linetype=:contourf,title = \"error 2\");\nplot(p1,p2,p3,p4,p5)\n","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: neural_adapter)","category":"page"},{"location":"pinn/neural_adapter/#Domain-decomposition","page":"Transfer Learning with Neural Adapter","title":"Domain decomposition","text":"","category":"section"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"In this example, we first obtain a prediction of 2D Poisson equation on subdomains. We split up full domain into 10 sub problems by x, and create separate neural networks for each sub interval. If x domain ∈ [x0, xend] so, it is decomposed on 10 part: sub x domains = {[x0, x1], ... [xi,xi+1], ..., x9,xend]}. And then using the method neural_adapter, we retrain the banch of 10 predictions to the one prediction for full domain of task.","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: domain_decomposition)","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n\n# Space\nx_0 = 0.0\nx_end = 1.0\nx_domain = Interval(x_0, x_end)\ny_domain = Interval(0.0, 1.0)\ndomains = [x ∈ x_domain,\n           y ∈ y_domain]\n\ncount_decomp = 10\n\n# Neural network\naf = Flux.tanh\ninner = 10\nchains = [FastChain(FastDense(2, inner, af), FastDense(inner, inner, af), FastDense(inner, 1)) for _ in 1:count_decomp]\ninitθs = map(c -> Float64.(c), DiffEqFlux.initial_params.(chains))\n\nxs_ = infimum(x_domain):1/count_decomp:supremum(x_domain)\nxs_domain = [(xs_[i], xs_[i+1]) for i in 1:length(xs_)-1]\ndomains_map = map(xs_domain) do (xs_dom)\n    x_domain_ = Interval(xs_dom...)\n    domains_ = [x ∈ x_domain_,\n                y ∈ y_domain]\nend\n\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\nfunction create_bcs(bcs,x_domain_,phi_bound)\n    x_0, x_e =  x_domain_.left, x_domain_.right\n    if x_0 == 0.0\n        bcs = [u(0,y) ~ 0.0,\n               u(x_e,y) ~ analytic_sol_func(x_e,y),\n               u(x,0) ~ 0.0,\n               u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n        return bcs\n    end\n    bcs = [u(x_0,y) ~ phi_bound(x_0,y),\n           u(x_e,y) ~ analytic_sol_func(x_e,y),\n           u(x,0) ~ 0.0,\n           u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n    bcs\nend\n\nreses = []\nphis = []\npde_system_map = []\n\nfor i in 1:count_decomp\n    println(\"decomposition $i\")\n    domains_ = domains_map[i]\n    phi_in(cord) = phis[i-1](cord,reses[i-1].minimizer)\n    phi_bound(x,y) = phi_in(vcat(x,y))\n    @register phi_bound(x,y)\n    Base.Broadcast.broadcasted(::typeof(phi_bound), x,y) = phi_bound(x,y)\n    bcs_ = create_bcs(bcs,domains_[1].domain, phi_bound)\n    @named pde_system_ = PDESystem(eq, bcs_, domains_, [x, y], [u(x, y)])\n    push!(pde_system_map,pde_system_)\n    strategy = NeuralPDE.GridTraining([0.1/count_decomp, 0.1])\n\n    discretization = NeuralPDE.PhysicsInformedNN(chains[i], strategy; init_params=initθs[i])\n\n    prob = NeuralPDE.discretize(pde_system_,discretization)\n    symprob = NeuralPDE.symbolic_discretize(pde_system_,discretization)\n    res_ = GalacticOptim.solve(prob, BFGS(), maxiters=1000)\n    phi = discretization.phi\n    push!(reses, res_)\n    push!(phis, phi)\nend\n\nfunction compose_result(dx)\n    u_predict_array = Float64[]\n    diff_u_array = Float64[]\n    ys = infimum(domains[2].domain):dx:supremum(domains[2].domain)\n    xs_ = infimum(x_domain):dx:supremum(x_domain)\n    xs = collect(xs_)\n    function index_of_interval(x_)\n        for (i,x_domain) in enumerate(xs_domain)\n            if x_<= x_domain[2] && x_>= x_domain[1]\n                return i\n            end\n        end\n    end\n    for x_ in xs\n        i = index_of_interval(x_)\n        u_predict_sub = [first(phis[i]([x_,y],reses[i].minimizer)) for y in ys]\n        u_real_sub = [analytic_sol_func(x_,y)  for y in ys]\n        diff_u_sub = abs.(u_predict_sub .- u_real_sub)\n        append!(u_predict_array,u_predict_sub)\n        append!(diff_u_array,diff_u_sub)\n    end\n    xs,ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\n    u_predict = reshape(u_predict_array,(length(xs),length(ys)))\n    diff_u = reshape(diff_u_array, (length(xs),length(ys)))\n    u_predict, diff_u\nend\ndx= 0.01\nu_predict, diff_u = compose_result(dx)\n\n\ninner_ = 18\naf = Flux.tanh\nchain2 = FastChain(FastDense(2,inner_,af),\n                   FastDense(inner_,inner_,af),\n                   FastDense(inner_,inner_,af),\n                   FastDense(inner_,inner_,af),\n                   FastDense(inner_,1))\n\ninitθ2 =Float64.(DiffEqFlux.initial_params(chain2))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\n\nlosses = map(1:count_decomp) do i\n    loss(cord,θ) = chain2(cord,θ) .- phis[i](cord,reses[i].minimizer)\nend\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nprob_ = NeuralPDE.neural_adapter(losses,initθ2, pde_system_map,NeuralPDE.GridTraining([0.1/count_decomp,0.1]))\nres_ = GalacticOptim.solve(prob_, BFGS();cb=cb, maxiters=2000)\nprob_ = NeuralPDE.neural_adapter(losses,res_.minimizer, pde_system_map, NeuralPDE.GridTraining([0.05/count_decomp,0.05]))\nres_ = GalacticOptim.solve(prob_, BFGS();cb=cb,  maxiters=1000)\n\nparameterless_type_θ = DiffEqBase.parameterless_type(initθ2)\nphi_ = NeuralPDE.get_phi(chain2,parameterless_type_θ)\n\nxs,ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_ = reshape([first(phi_([x,y],res_.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\n\np1 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict 1\");\np2 = plot(xs, ys, u_predict_,linetype=:contourf,title = \"predict 2\");\np3 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np4 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error 1\");\np5 = plot(xs, ys, diff_u_,linetype=:contourf,title = \"error 2\");\nplot(p1,p2,p3,p4,p5)","category":"page"},{"location":"pinn/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: decomp)","category":"page"},{"location":"solvers/full_kolmogorov/#Full-Kolmogorov-PDE-Solver","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"","category":"section"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The full Kolmogorov PDE solver obtains the complete solution of a family of γ γ-parameterised Kolmogorov PDE of the form, (Image: )","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The  σ and μ are also parameterised by γ. Thus before we define the neural network solution, we figure out the dimensions required. Let's say the domain of the PDE is (Image: ) The first dimension is taken by t , and the next d dimensions are taken by x<sup>d</sup> i.e. spatial region. The next dimensions are for γ that parameterise the  σ, μ and phi (the initial condition). And thus, (Image: ) [a<sub>1</sub> | a<sub>2</sub> ....  | a<sub>n</sub>]   represents  horizontal concatenation. So for defining γ we will be defining :","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"σ_γ can be atmost d + 1 matrices with dimensions d x d  or nothing.\nμ_γ  can comprise of either ad x d matrix and a matrix of dimensions d x 1 or both, or nothing.\nphi_γ can be a matrix with dimensions k x 1.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We define the prototypes for the above parameters:","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"γ_sigma_prototype  should be a 3 dimensional matrix with first two dimensions d and d, and the last dimension should be number of matrices (at most d + 1) or it can be nothing.\nγ_mu_prototype  should comprise of 2 matrices one with dimensions d x dand other with dimensions d x 1. So we define γ_mu_prototype as (matrix1 , matrix2) we put nothing in the place if we dont require either of them. Or if we dont want it parameterised, we define,  γ_mu_prototype as nothing\nγ_phi_prototype should be a matrix with k x 1 dimensions.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"Now lets define a test problem,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We start by defining the dimension d of the solution and the prototypes for the problem.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"d = 1\nγ_mu_prototype = nothing\nγ_sigma_prototype = zeros(d , d , 1)\nγ_phi_prototype = nothing","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And now the neural network solution with the number of dimensions we require at input.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"m = Chain(Dense(3 , 16, elu),Dense(16, 16, elu) , Dense(16 , 5 , elu) , Dense(5 , 1))","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we write the domains of t , x, γ_sigma, γ_mu and γ_phi. And thus we use KolmogorovParamDomain to pass the domains of   γ_sigma, γ_mu and γ_phi as arguments, respectively.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"tspan = (0.00 , 1.00)\nxspan = (0.00 , 3.00)\ny_domain = KolmogorovParamDomain((0.00 , 2.00) , (0.00 , 0.00) , (0.00 , 0.00) )","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The functions σ(x , γ_sigma) where  and μ(x , γ_mu1, γ_mu2) (γ_mu1 is the d x d x 1 matrix and γ_mu2 is a d x 1 x 1 matrix)","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"g(x , γ_sigma) = γ_sigma[: , : , 1] #the σ(x , γ_sigma)\nf(x , γ_mu_1 ,γ_mu_2 ) = [0.00] # the μ(x , γ_mu1, γ_mu2)","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we define the function phi(x , γ_phi) where γ_phi is a matrix with k x 1 x trajectories . Thus the function should return an array with dimensions 1 x trajectories.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"function phi(x , y_phi)\n  x.^2\nend","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We define the dt , dx and dy for the equation,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"dt = 0.01\ndx = 0.01\ndy = 0.01","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we finally define the optimiser, the algorithms for SDE solution and ensemble simulation.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"opt = Flux.ADAM(1e-2)\nsdealg = EM()\nensemblealg = EnsembleThreads()","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"Finally we define the ParamKolmogorovPDEProblem ,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"prob = ParamKolmogorovPDEProblem(f , g , phi , xspan , tspan , d  , y_domain  ; Y_sigma_prototype = γ_sigma_prototype)\nsol = solve(prob, NNParamKolmogorov(m,opt , sdealg,ensemblealg) , verbose = true, dt = 0.01,\n            abstol=1e-10, dx = 0.01 , trajectories = trajectories ,  maxiters = 150 , use_gpu = false)","category":"page"},{"location":"examples/optimal_stopping_american/#Optimal-Stopping-Times-of-American-Options","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"","category":"section"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Here, we will aim to solve an optimal stopping problem using the NNStopping algorithm.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Let us consider standard American options. Unlike European options, American options can be exercized before their maturity and thus the problem reduces to finding an optimal stopping time.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"As stated above, since we can execute the option at any optimal time before the maturity of the option, the standard Black-Scholes model gets modified to:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"  fracVt + rSfracVS + frac12σ^2S^2frac^2 VS^2 -rV  0","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The stock price will follow a standard geometric brownian motion given by:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"  dS_t = rS_tdt + σS_tdW_t","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"And thus our final aim will be to calculate:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"(Image: american_option)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We will be using a SDEProblem to denote a problem of this type. We can define this as a SDEProblem and add a terminal condition g in order to price the American Options.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We will take the case of an American max put option with strike price K, constant volatility β, a risk-free rate r, the initial stock price u0 = 80.00, the maturity T, and number of steps N. The forcing function f and noise function sigma are defined for the type of model. See StochasticDiffEq documentation.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"d = 1 #Dimensions of initial stock price\nr = 0.04f0\nbeta = 0.2f0\nK = 100.00\nT = 1.0\nu0 = fill(80.00 , d , 1) #Initial Stock Price\n#Defining the drift (f) and diffusion(sigma)\nf(du,u,p,t) = (du .= r*u)\nsigma(du,u,p,t)  = (du .= Diagonal(beta*u))\n\ntspan = (0.0 , T)\nN = 50\ndt = tspan[2]/(N - 1)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The final part is the payoff function:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"(Image: payoff_func)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The discounted payoff function is:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"function g(t , x)\n  return exp(-r*t)*(max(K -  maximum(x)  , 0))\nend","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Now, in order to define an optimal stopping problem, we will use the SDEProblem and pass the discounted payoff function g as an kwarg.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"prob  = SDEProblem(f , sigma , u0 , tspan ; g = g)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Finally, let's build our neural network model using Flux.jl. Note that the final layer should be the softmax (Flux.softmax) function as we need the sum of probabilities at all stopping times to be 1. And then add an optimizer function.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"m = Chain(Dense(d , 5, tanh), Dense(5, 16 , tanh)  , Dense(16 , N ), softmax)\nopt = Flux.ADAM(0.1)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We add algorithms to solve the SDE and the Ensemble. These are the algorithms required to solve the SDEProblem (we use the Euler-Maruyama algorithm in this case) and the EnsembleProblem to run multiple simulations. See Ensemble Algorithms.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"sdealg = EM()\nensemblealg = EnsembleThreads()","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Finally, we call the solve function:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"sol = solve(prob, NeuralPDE.NNStopping( m, opt , sdealg , ensemblealg), verbose = true, dt = dt,\n            abstol=1e-6, maxiters = 20 , trajectories = 200)\n","category":"page"},{"location":"pinn/2D/#dimensional-PDEs-with-GPU","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"","category":"section"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"the 2-dimensional PDE:","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"_t u(x y t) = ^2_x u(x y t) + ^2_y u(x y t)  ","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"with the initial and boundary conditions:","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"beginalign*\nu(x y 0) = e^x+y cos(x + y)       \nu(0 y t) = e^y   cos(y + 4t)      \nu(2 y t) = e^2+y cos(2 + y + 4t)  \nu(x 0 t) = e^x   cos(x + 4t)      \nu(x 2 t) = e^x+2 cos(x + 2 + 4t)  \nendalign*","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"on the space and time domain:","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"x in 0 2   y in 0 2    t in 0 2  ","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"with physics-informed neural networks.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature, Cuba, CUDA, QuasiMonteCarlo\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min= 0.\nt_max = 2.0\nx_min = 0.\nx_max = 2.\ny_min = 0.\ny_max = 2.\n\n# 2D PDE\neq  = Dt(u(t,x,y)) ~ Dxx(u(t,x,y)) + Dyy(u(t,x,y))\n\nanalytic_sol_func(t,x,y) = exp(x+y)*cos(x+y+4t)\n# Initial and boundary conditions\nbcs = [u(t_min,x,y) ~ analytic_sol_func(t_min,x,y),\n       u(t,x_min,y) ~ analytic_sol_func(t,x_min,y),\n       u(t,x_max,y) ~ analytic_sol_func(t,x_max,y),\n       u(t,x,y_min) ~ analytic_sol_func(t,x,y_min),\n       u(t,x,y_max) ~ analytic_sol_func(t,x,y_max)]\n\n# Space and time domains\ndomains = [t ∈ Interval(t_min,t_max),\n           x ∈ Interval(x_min,x_max),\n           y ∈ Interval(y_min,y_max)]\n\n# Neural network\ninner = 25\nchain = FastChain(FastDense(3,inner,Flux.σ),\n                  FastDense(inner,inner,Flux.σ),\n                  FastDense(inner,inner,Flux.σ),\n                  FastDense(inner,inner,Flux.σ),\n                  FastDense(inner,1))\n\ninitθ = CuArray(Float64.(DiffEqFlux.initial_params(chain)))\n\nstrategy = GridTraining(0.05)\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy;\n                                   init_params = initθ)\n\n@named pde_system = PDESystem(eq,bcs,domains,[t,x,y],[u(t, x, y)])\nprob = discretize(pde_system,discretization)\nsymprob = symbolic_discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob,ADAM(0.01);cb=cb,maxiters=2500)","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"The remake function allows to rebuild the PDE problem.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,ADAM(0.001);cb=cb,maxiters=2500)\n\nphi = discretization.phi\nts,xs,ys = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]\nu_real = [analytic_sol_func(t,x,y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi([t, x, y], res.minimizer))) for t in ts for x in xs for y in ys]\n\n\nusing Plots\nusing Printf\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t,x,y) for x in xs for y in ys], (length(xs),length(ys)))\n        u_predict = reshape([Array(phi([t, x, y], res.minimizer))[1] for x in xs for y in ys], length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict, t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st=:contourf,label=\"\", title=title)\n        plot(p1,p2,p3)\n    end\n    gif(anim,\"3pde.gif\", fps=10)\nend\n\nplot_(res)\n","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: 3pde)","category":"page"},{"location":"pinn/2D/#Performance-benchmarks","page":"2-dimensional PDEs with GPU","title":"Performance benchmarks","text":"","category":"section"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"Here are some performance benchmarks for 2d-pde with various number of input points and the number of neurons in the hidden layer, measuring the time for 100 iterations. Сomparing runtime with GPU and CPU.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"\njulia> CUDA.device()\nCuDevice(0): Tesla P100-PCIE-16GB\n","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: image)","category":"page"},{"location":"pinn/3rd/#ODE-with-a-3rd-Order-Derivative","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"","category":"section"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"Let's consider the ODE with a 3rd-order derivative:","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"beginalign*\n^3_x u(x) = cos(pi x)  \nu(0) = 0  \nu(1) = cos(pi)  \n_x u(0) = 1  \nx in 0 1  \nendalign*","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We will use physics-informed neural networks.","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x\n@variables u(..)\n\nDxxx = Differential(x)^3\nDx = Differential(x)\n# ODE\neq = Dxxx(u(x)) ~ cos(pi*x)\n\n# Initial and boundary conditions\nbcs = [u(0.) ~ 0.0,\n       u(1.) ~ cos(pi),\n       Dx(u(1.)) ~ 1.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0)]\n\n# Neural network\nchain = FastChain(FastDense(1,8,Flux.σ),FastDense(8,1))\n\ndiscretization = PhysicsInformedNN(chain, QuasiRandomTraining(20))\n@named pde_system = PDESystem(eq,bcs,domains,[x],[u(x)])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, ADAM(0.01); cb = cb, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We can plot the predicted solution of the ODE and its analytical solution.","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using Plots\n\nanalytic_sol_func(x) = (π*x*(-x+(π^2)*(2*x-3)+1)-sin(π*x))/(π^3)\n\ndx = 0.05\nxs = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.minimizer)) for x in xs]\n\nx_plot = collect(xs)\nplot(x_plot ,u_real,title = \"real\")\nplot!(x_plot ,u_predict,title = \"predict\")","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"(Image: hodeplot)","category":"page"},{"location":"pinn/poisson/#Poisson-Equation","page":"Poisson Equation","title":"Poisson Equation","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"In this example, we will solve a Poisson equation:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"^2_x u(x y) + ^2_y u(x y) = - sin(pi x) sin(pi y)  ","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"with the boundary conditions:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"beginalign*\nu(0 y) = 0  \nu(1 y) = - sin(pi) sin(pi y)  \nu(x 0) = 0  \nu(x 1) =  - sin(pi x) sin(pi)  \nendalign*","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"on the space domain:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"x in 0 1    y in 0 1  ","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"with grid discretization dx = 0.1. We will use physics-informed neural networks.","category":"page"},{"location":"pinn/poisson/#Copy-Pastable-Code","page":"Poisson Equation","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.f0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.f0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]\n\n# Neural network\ndim = 2 # number of dimensions\nchain = FastChain(FastDense(dim,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\n# Initial parameters of Neural network\ninitθ = Float64.(DiffEqFlux.initial_params(chain))\n\n# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain,GridTraining(dx),init_params =initθ)\n\n@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = discretize(pde_system,discretization)\n\n#Optimizer\nopt = Optim.BFGS()\n\n#Callback function\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, opt, cb = cb, maxiters=1000)\nphi = discretization.phi\n\nusing Plots\n\nxs,ys = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/poisson/#Detailed-Description","page":"Poisson Equation","title":"Detailed Description","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"The ModelingToolkit PDE interface for this example looks like this:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.f0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.f0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we define the neural network, where the input of NN equals the number of dimensions and output equals the number of equations in the system.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"# Neural network\ndim = 2 # number of dimensions\nchain = FastChain(FastDense(dim,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Convert weights of neural network from Float32 to Float64 in order to all inner calculation will be with Float64.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"# Initial parameters of Neural network\ninitθ = Float64.(DiffEqFlux.initial_params(chain))","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we build PhysicsInformedNN algorithm where dx is the step of discretization, strategy stores information for choosing a training strategy and init_params =initθ initial parameters of neural network.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx),init_params =initθ)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"As described in the API docs, we now need to define the PDESystem and create PINNs problem using the discretize method.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = discretize(pde_system,discretization)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we define the callback function and the optimizer. And now we can solve the PDE using PINNs (with the number of epochs maxiters=1000).","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"#Optimizer\nopt = Optim.BFGS()\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, opt, cb = cb, maxiters=1000)\nphi = discretization.phi","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"xs,ys = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"(Image: poissonplot)","category":"page"},{"location":"examples/ode/#Solving-ODEs-with-Neural-Networks","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"","category":"section"},{"location":"examples/ode/","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"The following is an example of solving a DifferentialEquations.jl ODEProblem with a neural network using the physics-informed neural networks approach specialized to 1-dimensional PDEs (ODEs).","category":"page"},{"location":"examples/ode/","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"using Flux, Optim\nusing NeuralPDE\n# Run a solve on scalars\nlinear = (u, p, t) -> cos(2pi * t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0, tspan)\nchain = Flux.Chain(Dense(1, 5, σ), Dense(5, 1))\nopt = Flux.ADAM(0.1, (0.9, 0.95))\n@time sol = solve(prob, NeuralPDE.NNODE(chain, opt), dt=1 / 20f0, verbose=true,\n            abstol=1e-10, maxiters=200)","category":"page"},{"location":"pinn/heterogeneous/#Differential-Equations-with-Heterogeneous-Inputs","page":"Differential Equations with Heterogeneous Inputs","title":"Differential Equations with Heterogeneous Inputs","text":"","category":"section"},{"location":"pinn/heterogeneous/","page":"Differential Equations with Heterogeneous Inputs","title":"Differential Equations with Heterogeneous Inputs","text":"A differential equation is said to have heterogeneous inputs when its dependent variables depend on different independent variables:","category":"page"},{"location":"pinn/heterogeneous/","page":"Differential Equations with Heterogeneous Inputs","title":"Differential Equations with Heterogeneous Inputs","text":"u(x) + w(x v) = fracpartial w(x v)partial w","category":"page"},{"location":"pinn/heterogeneous/","page":"Differential Equations with Heterogeneous Inputs","title":"Differential Equations with Heterogeneous Inputs","text":"Here, we write an arbitrary heterogeneous system:","category":"page"},{"location":"pinn/heterogeneous/","page":"Differential Equations with Heterogeneous Inputs","title":"Differential Equations with Heterogeneous Inputs","text":"@parameters x y\n@variables p(..) q(..) r(..) s(..)\nDx = Differential(x)\nDy = Differential(y)\n\n# 2D PDE\neq  = p(x) + q(y) + Dx(r(x, y)) + Dy(s(y, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [p(1) ~ 0.f0, q(-1) ~ 0.0f0,\n       r(x, -1) ~ 0.f0, r(1, y) ~ 0.0f0,\n       s(y, 1) ~ 0.0f0, s(-1, x) ~ 0.0f0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           y ∈ Interval(0.0, 1.0)]\n\nnumhid = 3\nfastchains = [[FastChain(FastDense(1, numhid, Flux.σ), FastDense(numhid, numhid, Flux.σ), FastDense(numhid, 1)) for i in 1:2];\n                        [FastChain(FastDense(2, numhid, Flux.σ), FastDense(numhid, numhid, Flux.σ), FastDense(numhid, 1)) for i in 1:2]]\ndiscretization = NeuralPDE.PhysicsInformedNN(fastchains, QuadratureTraining()\n\n@named pde_system = PDESystem(eq, bcs, domains, [x,y], [p(x), q(y), r(x, y), s(y, x)])\nprob = SciMLBase.discretize(pde_system, discretization)\nres = GalacticOptim.solve(prob, BFGS(); cb=cb, maxiters=100)","category":"page"},{"location":"examples/kolmogorovbackwards/#Solving-Kolmogorov-Equations-with-Neural-Networks","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"","category":"section"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"A Kolmogorov PDE is of the form :","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: KPDE)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Consider S to be a solution process to the SDE:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: StochasticP)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"then the solution to the Kolmogorov PDE is given as:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: Solution)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"A Kolmogorov PDE Problem can be defined using a SDEProblem:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"SDEProblem(μ,σ,u0,tspan,xspan,d)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, u0 is the initial distribution of x. Here, we define u(0,x) as the probability density function of u0.μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x. u0 can be defined using Distributions.jl.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Another way of defining a KolmogorovPDE is to use the KolmogorovPDEProblem.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"KolmogorovPDEProblem(μ,σ,phi,tspan,xspan,d)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, phi is the initial condition on u(t,x) when t = 0. μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"To solve this problem use:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"NNKolmogorov(chain, opt , sdealg): Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, chain is a Flux.jl chain with a d-dimensional input and a 1-dimensional output.opt is a Flux.jl optimizer. And sdealg is a high-order algorithm to calculate the solution for the SDE, which is used to define the learning data for the problem. Its default value is the classic Euler-Maruyama algorithm.","category":"page"},{"location":"solvers/pinns/#Physics-Informed-Neural-Networks","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Using the PINNs solver, we can solve general nonlinear PDEs:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"(Image: generalPDE)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"with suitable boundary conditions:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"(Image: bcs)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"where time t is a special component of x, and Ω contains the temporal domain.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"PDEs are defined using the ModelingToolkit.jl PDESystem:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"@named pde_system = PDESystem(eq,bcs,domains,param,var)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Here, eq is the equation, bcs represents the boundary conditions, param is the parameter of the equation (like [x,y]), and var represents variables (like [u]).","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"The PhysicsInformedNN discretizer is defined as:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"discretization = PhysicsInformedNN(chain,\n                                   strategy;\n                                   init_params = nothing,\n                                   phi = nothing,\n                                   derivative = nothing,\n                                   )","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Keyword arguments:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"chain is a Flux.jl chain, where the input of NN equals the number of dimensions and output equals the number of equations in the system,\nstrategy determines which training strategy will be used,\ninit_params is the initial parameter of the neural network. If nothing then automatically generated from the neural network,\nphi is a trial solution,\nderivative is a method that calculates the derivative.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"The method discretize interprets from the ModelingToolkit PDE form to the PINNs Problem.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"prob = discretize(pde_system, discretization)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"which outputs an OptimizationProblem for GalacticOptim.jl.","category":"page"},{"location":"solvers/pinns/#Training-strategy","page":"Physics-Informed Neural Networks","title":"Training strategy","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"List of training strategies that are available now:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"GridTraining(dx): Initialize points on a lattice uniformly spaced via dx. If dx is a scalar, then dx corresponds to the spacing in each direction. If dx is a vector, then it should be sized to match the number of dimensions and corresponds to the spacing per direction.\nStochasticTraining(points:bcs_points = ponits): points is number of stochastically sampled points from the domain,  bcs_points is number of points for boundary conditions(by default, it equals points).    In each optimization iteration, we randomly select a new subset of points from a full training set.\nQuasiRandomTraining(points;bcs_points = ponits, sampling_alg = UniformSample(), resampling = true, minibatch=500): The training set is generated on quasi-random low discrepency sequences. points is the number of quasi-random points in every subset or set, bcs_points is number of points for boundary conditions(by default, it equals points), sampling_alg is the quasi-Monte Carlo sampling algorithm. if resampling = false, the full training set is generated in advance before training, and at each iteration, one subset is randomly selected out of the batch.minibatch is the number of subsets in full training set. The number of the total points is length(lb) * points * minibatch, where lb is the lower bound and length(lb) is the dimensionality. if resampling = true, the training set isn't generated beforehand, and one set of quasi-random points is generated directly at each iteration in runtime. In this case minibatch has no effect.\nSee the QuasiMonteCarlo.jl for the full set of quasi-random sampling algorithms which are available.\nQuadratureTraining(;quadrature_alg=CubatureJLh(),reltol= 1e-6,abstol= 1e-3,maxiters=1e3,batch=100):","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"The loss is computed as an approximation of the integral of the PDE loss   at each iteration using adaptive quadrature methods   via the differentiable Quadrature.jl.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"quadrature_alg is quadrature algorithm,\nreltol: relative tolerance,\nabstol: absolute tolerance,\nmaxiters: the maximum number of iterations in quadrature algorithm,\nbatch: the preferred number of points to batch. If batch = 0, the number of points in the batch is determined adaptively by the algorithm.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"See the Quadrature.jl documentation for the choices of quadrature methods.","category":"page"},{"location":"solvers/pinns/#Transfer-Learning-with-neural_adapter","page":"Physics-Informed Neural Networks","title":"Transfer Learning with neural_adapter","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"neural_adapter(loss_function,initθ,pde_system,strategy): the method that trains a neural network using the results from one already obtained prediction.   Keyword arguments:\nloss_function:the body of loss function,\ninitθ: the initial parameter of new neural networks,\npde_system: PDE are defined using the ModelingToolkit.jl ,\nstrategy: determines which training strategy will be used,\nneural_adapter(loss_functions::Array,initθ,pde_systems::Array,strategy): the method that trains a neural network using the results from many already obtained predictions. Keyword arguments:\nloss_functions: the body of loss functions,\ninitθ: the initial parameter of the neural network,\npde_systems: PDEs are defined using the ModelingToolkit.jl,\nstrategy: determines which training strategy will be used.","category":"page"},{"location":"solvers/pinns/#Low-level-API","page":"Physics-Informed Neural Networks","title":"Low-level API","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"These additional methods exist to help with introspection:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"symbolic_discretize(pde_system,discretization): This method is the same as discretize but instead returns the unevaluated Julia function to allow the user to see the generated training code.\nbuild_symbolic_loss_function(eqs,indvars,depvars, phi, derivative, initθ; bc_indvars=nothing): return symbolic inner representation for the loss function.   Keyword arguments:\neqs: equation or equations,\nindvars: independent variables (the parameter of the equation),\ndepvars: dependent variables,\nphi:trial solution,\nderivative: method that calculates the derivative,\ninitθ: the initial parameter of the neural network,\nbc_indvars: independent variables for each boundary conditions.\nbuild_symbolic_equation(eq,indvars,depvars): return symbolic inner representation for the equation.\nbuild_loss_function(eqs, indvars, depvars, phi, derivative, initθ; bc_indvars=nothing): returns the body of loss function, which is the executable Julia function, for the main equation or boundary condition.\nget_loss_function(loss_functions, train_sets, strategy::TrainingStrategies; τ = nothing): return the executable loss function.  Keyword arguments:\nloss_functions: the body of loss function, which is created using  build_loss_function,\ntrain_sets: training sets,\nstrategy: training strategy,\nτ: normalizing coefficient for loss function. If τ is nothing, then it is automatically set to 1/n where n is the number of points checked in the loss function.\nget_phi(chain): return function for trial solution.\nget_numeric_derivative(): return method that calculates the derivative.\ngenerate_training_sets(domains,dx,bcs,_indvars::Array,_depvars::Array): return training sets for equations and boundary condition, that is used for GridTraining strategy.\nget_variables(eqs,_indvars::Array,_depvars::Array): returns all variables that are used in each equations or boundary condition.\nget_argument(eqs,_indvars::Array,_depvars::Array): returns all arguments that are used in each equations or boundary condition.\nget_bounds(domains,bcs,_indvars::Array,_depvars::Array): return pairs with lower and upper bounds for all domains. It is used for all non-grid training strategy: StochasticTraining, QuasiRandomTraining, QuadratureTraining.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"See how this can be used in Debugging section or 2-D Burgers equation, low-level API  examples.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/#Neural-Network-Solvers-for-Kolmogorov-Backwards-Equations","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"","category":"section"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"A Kolmogorov PDE is of the form :","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Considering S to be a solution process to the SDE:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"then the solution to the Kolmogorov PDE is given as:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"A Kolmogorov PDE Problem can be defined using a SDEProblem:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"SDEProblem(μ,σ,u0,tspan,xspan,d)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, u0 is the initial distribution of x. Here, we define u(0,x) as the probability density function of u0.μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x. u0 can be defined using Distributions.jl.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Another way of defining a KolmogorovPDE is to use the KolmogorovPDEProblem.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"KolmogorovPDEProblem(μ,σ,phi,tspan,xspan,d)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, phi is the initial condition on u(t,x) when t = 0. μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"To solve this problem use:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"NNKolmogorov(chain, opt , sdealg): Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, chain is a Flux.jl chain with a d-dimensional input and a 1-dimensional output.opt is a Flux.jl optimizer. And sdealg is a high-order algorithm to calculate the solution for the SDE, which is used to define the learning data for the problem. Its default value is the classic Euler-Maruyama algorithm.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/#Using-GPU-for-Kolmogorov-Equations","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Using GPU for Kolmogorov Equations","text":"","category":"section"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"For running Kolmogorov Equations on a GPU, there are certain aspects that are need to be taken care of:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Convert the model parameters to CuArrays using the fmap function given by Flux.jl:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"m = Chain(Dense(1, 64, σ), Dense(64, 64, σ) , Dense(5, 2))\nm = fmap(cu, m)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Unlike other solvers, we need to specify explicitly that the solver is to run on the GPU. This can be done by passing the use_gpu = true into the solver.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"solve(prob, NeuralPDE.NNKolmogorov(m, opt, sdealg, ensemblealg), use_gpu = true,  verbose = true, dt = dt, dx = dx , trajectories = trajectories , abstol=1e-6, maxiters = 1000)","category":"page"},{"location":"#NeuralPDE.jl:-Scientific-Machine-Learning-for-Partial-Differential-Equations","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning for Partial Differential Equations","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"NeuralPDE.jl is a solver package which consists of neural network solvers for partial differential equations using scientific machine learning (SciML) techniques such as physics-informed neural networks (PINNs) and deep BSDE solvers. This package utilizes deep neural networks and neural stochastic differential equations to solve high-dimensional PDEs at a greatly reduced cost and greatly increased generality compared with classical methods.","category":"page"},{"location":"#Features","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"Features","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"Physics-Informed Neural Networks for automated PDE solving\nForward-Backwards Stochastic Differential Equation (FBSDE) methods for parabolic PDEs\nDeep-learning-based solvers for optimal stopping time and Kolmogorov backwards equations","category":"page"},{"location":"#Citation","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"Citation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"If you use NeuralPDE.jl in your research, please cite this paper:","category":"page"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"@article{zubov2021neuralpde,\n  title={NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},\n  author={Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luj{\\'a}n, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and others},\n  journal={arXiv preprint arXiv:2107.09443},\n  year={2021}\n}","category":"page"}]
}
