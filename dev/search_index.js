var documenterSearchIndex = {"docs":
[{"location":"examples/blackscholes/#Solving-the-100-dimensional-Black-Scholes-Barenblatt-Equation","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Black Scholes equation is a model for stock option price. In 1973, Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model, which is widely used in financing engineering for computing the option price over time. [1.] In this example, we will solve a Black-Scholes-Barenblatt equation of 100 dimensions. The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation, which models uncertain volatility and interest rates derived from the Black-Scholes equation. This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"To solve it using the TerminalPDEProblem, we write:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"d = 100 # number of dimensions\nX0 = repeat([1.0f0, 0.5f0], div(d,2)) # initial value of stochastic state\ntspan = (0.0f0,1.0f0)\nr = 0.05f0\nsigma = 0.4f0\nf(X,u,σᵀ∇u,p,t) = r * (u - sum(X.*σᵀ∇u))\ng(X) = sum(X.^2)\nμ_f(X,p,t) = zero(X) #Vector d x 1\nσ_f(X,p,t) = Diagonal(sigma*X) #Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d-dimensional -> 1-dimensional chain, while σᵀ∇u needs to be d+1-dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"hls  = 10 + d #hide layer size\nopt = Flux.ADAM(0.001)\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\nσᵀ∇u = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"And now we solve the PDE. Here, we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 150 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-6.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"ans = solve(prob, pdealg, verbose=true, maxiters=150, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-6)","category":"page"},{"location":"examples/blackscholes/#Reference","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Reference","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Shinde, A. S., and K. C. Takale. \"Study of Black-Scholes model and its applications.\" Procedia Engineering 38 (2012): 270-279.","category":"page"},{"location":"examples/nnrode_example/#Solving-Random-Ordinary-Differential-Equations","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"In this tutorial we will solve a RODE with NNRODE. Consider the equation:","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"du = f(uptW)dt","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"where f(uptW)=2usin(W) and W(t) is a Noise process.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"f = (u,p,t,W) ->   2u*sin(W)\ntspan = (0.00f0, 1.00f0)\nu0 = 1.0f0\ndt = 1/20f0","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"We start off by defining the NoiseProcess W(t). In this case, we define a simple Gaussian Process. See Noise Processes for defining other types of processes.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"W = WienerProcess(0.0,0.0,nothing)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"Then, we need to define our model. In order to define a model, we can use Flux.chain or DiffEqFlux.FastChain.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"chain = Flux.Chain(Dense(2,5,elu),Dense(5,1)) #Model using Flux","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"chain = FastChain(FastDense(2,50,tanh), FastDense(50,2)) #Model using DiffEqFlux","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"And let's define our optimizer function:","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"opt = ADAM(1e-3)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"Now, let's pass all the parameters to the algorithm and then call the solver. If we already have some initial parameters, we can pass them into the NNRODE as well.","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"alg = NNRODE(chain , W , opt , init_params)","category":"page"},{"location":"examples/nnrode_example/","page":"Solving Random Ordinary Differential Equations","title":"Solving Random Ordinary Differential Equations","text":"sol = solve(prob, NeuralPDE.NNRODE(chain,W,opt), dt=dt, verbose = true,\n            abstol=1e-10, maxiters = 15000)","category":"page"},{"location":"pinn/debugging/#Debugging-PINN-Solutions","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"","category":"section"},{"location":"pinn/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"Let's walk through debugging functions for the physics-informed neural network PDE solvers.","category":"page"},{"location":"pinn/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"using NeuralPDE, ModelingToolkit, Flux, DiffEqFlux, Zygote\n# 2d wave equation, neumann boundary condition\n@parameters x, t\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n#2D PDE\nC=1\neq  = Dtt(u(x,t)) ~ C^2*Dxx(u(x,t))\n\n# Initial and boundary conditions\nbcs = [u(0,t) ~ 0.,\n       u(1,t) ~ 0.,\n       u(x,0) ~ x*(1. - x),\n       Dt(u(x,0)) ~ 0. ]\n\n# Space and time domains\ndomains = [x ∈ IntervalDomain(0.0,1.0),\n           t ∈ IntervalDomain(0.0,1.0)]\n\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\ninitθ = DiffEqFlux.initial_params(chain)\n\nphi = NeuralPDE.get_phi(chain)\nderivative = NeuralPDE.get_numeric_derivative()\n\nu_ = (cord, θ, phi)->sum(phi(cord, θ))\n\nphi([1,2], initθ)\n\nphi_ = (p) -> phi(p, initθ)[1]\ndphi = Zygote.gradient(phi_,[1.,2.])\n\ndphi1 = derivative(phi,u_,[1.,2.],[[ 0.0049215667, 0.0]],1,initθ)\ndphi2 = derivative(phi,u_,[1.,2.],[[0.0,  0.0049215667]],1,initθ)\nisapprox(dphi[1][1], dphi1, atol=1e-8)\nisapprox(dphi[1][2], dphi2, atol=1e-8)\n\n\nindvars = [x,t]\ndepvars = [u]\ndim = length(domains)\ndx = 0.1\nstrategy = NeuralPDE.GridTraining(dx)\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq,indvars,depvars,phi, derivative,chain,initθ,strategy)\n\njulia> expr_pde_loss_function = NeuralPDE.build_symbolic_loss_function(eq,indvars,depvars,phi,derivative,chain,initθ,strategy)\n\n:((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667], [0.0, 0.0049215667]], 2, var\"##θ#529\") .- derivative.(phi, u, cord, Array{Float32,1}[[0.0049215667, 0.0], [0.0049215667, 0.0]], 2, var\"##θ#529\")\n              end\n          end\n      end)\n\njulia> bc_indvars = NeuralPDE.get_varibles(bcs,indvars,depvars)\n4-element Array{Array{Any,1},1}:\n [:t]\n [:t]\n [:x]\n [:x]\n\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars,\n                                                     phi, derivative,chain,initθ,strategy,\n                                                     bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n\njulia> expr_bc_loss_functions = [NeuralPDE.build_symbolic_loss_function(bc,indvars,depvars,\n                                                                        phi, derivative,chain,initθ,strategy,\n                                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n4-element Array{Expr,1}:\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- (*).(x, (+).(1.0, (*).(-1, x)))\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667]], 1, var\"##θ#529\") .- 0.0\n              end\n          end\n      end)\n\ntrain_sets = NeuralPDE.generate_training_sets(domains,dx,[eq],bcs,indvars,depvars)\npde_train_set,bcs_train_set = train_sets\n\njulia> pde_train_set\n1-element Array{Array{Float32,2},1}:\n [0.1 0.2 … 0.8 0.9; 0.1 0.1 … 1.0 1.0]\n\n\njulia> bcs_train_set\n4-element Array{Array{Float32,2},1}:\n [0.0 0.0 … 0.0 0.0; 0.0 0.1 … 0.9 1.0]\n [1.0 1.0 … 1.0 1.0; 0.0 0.1 … 0.9 1.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n\n\npde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains,[eq],bcs,indvars,depvars)\n\njulia> pde_bounds\n1-element Array{Array{Any,1},1}:\n [[0.0, 1.0], [0.0, 1.0]]\n\njulia> bcs_bounds\n4-element Array{Array{Any,1},1}:\n [0, [0.0, 1.0]]\n [1, [0.0, 1.0]]\n [[0.0, 1.0], 0]\n [[0.0, 1.0], 0]\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,strategy)\n\npde_system = PDESystem(eq,bcs,domains,indvars,depvars)\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nexpr_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\nexpr_pde_loss_function , expr_bc_loss_functions = expr_prob\n","category":"page"},{"location":"pinn/system/#Systems-of-PDEs","page":"Systems of PDEs","title":"Systems of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"In this example, we will solve the PDE system:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: pdesystem)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with the initial conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: Initial)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"and the boundary conditions:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: boundary)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"with physics-informed neural networks.","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature,Cubature\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),\n       Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),\n       0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]\n\nbcs = [u1(0,x) ~ sin(pi*x),\n       u2(0,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       u1(t,0) ~ 0.,\n       u2(t,0) ~ exp(-t),\n       u1(t,1) ~ 0.,\n       u2(t,1) ~ -exp(-t)]\n\n\n# Space and time domains\ndomains = [t ∈ IntervalDomain(0.0,1.0),\n           x ∈ IntervalDomain(0.0,1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 20\nchain1 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain2 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain3 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN([chain1,chain2,chain3], strategy)\n\npde_system = PDESystem(eqs,bcs,domains,[t,x],[u1,u2,u3])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob,BFGS(); cb = cb, maxiters=200)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,ADAM(10^-2); cb = cb, maxiters=10000)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,BFGS(); cb = cb, maxiters=200)\nphi = discretization.phi","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"And some analysis:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"using Plots\n\nts,xs = [domain.domain.lower:0.01:domain.domain.upper for domain in domains]\n\ninitθ = discretization.init_params\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers = [res.minimizer[s] for s in sep]\n\nanalytic_sol_func(t,x) = [exp(-t)*sin(pi*x), exp(-t)*cos(pi*x), (1+pi^2)*exp(-t)]\nu_real  = [[analytic_sol_func(t,x)[i] for t in ts for x in xs] for i in 1:3]\nu_predict  = [[phi[i]([t,x],minimizers[i])[1] for t in ts  for x in xs] for i in 1:3]\ndiff_u = [abs.(u_real[i] .- u_predict[i] ) for i in 1:3]\n\nfor i in 1:3\n    p1 = plot(ts, xs, u_real[i], st=:surface,title = \"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i], st=:surface,title = \"predict\");\n    p3 = plot(ts, xs, diff_u[i],linetype=:contourf,title = \"error\");\n    plot(p1,p2,p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: u1)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: u2)","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"(Image: u3)","category":"page"},{"location":"pinn/system/#Solving-Matrices-of-PDEs","page":"Systems of PDEs","title":"Solving Matrices of PDEs","text":"","category":"section"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"Also, in addition to systems, we can use the matrix form of PDEs:","category":"page"},{"location":"pinn/system/","page":"Systems of PDEs","title":"Systems of PDEs","text":"@parameters x y\n@variables u[1:2,1:2](..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# matrix PDE\neqs  = @. [(Dxx(u_(x,y)) + Dyy(u_(x,y))) for u_ in u] ~ -sin(pi*x)*sin(pi*y)*[0 1; 0 1]\n\n# Initial and boundary conditions\nbcs = [u[1](x,0) ~ x, u[2](x,0) ~ 2, u[3](x,0) ~ 3, u[4](x,0) ~ 4]","category":"page"},{"location":"solvers/nnrode/#Random-Ordinary-Differential-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"solvers/nnrode/","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"TODO","category":"page"},{"location":"pinn/wave/#D-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Let's solve this 1-dimensional wave equation:","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: wave)","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.1 and physics-informed neural networks.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Further, the solution of this equation with the given boundary conditions is presented.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n@parameters t, x\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC=1\neq  = Dtt(u(t,x)) ~ C^2*Dxx(u(t,x))\n\n# Initial and boundary conditions\nbcs = [u(t,0) ~ 0.,# for all t > 0\n       u(t,1) ~ 0.,# for all t > 0\n       u(0,x) ~ x*(1. - x), #for all 0 < x < 1\n       Dt(u(0,x)) ~ 0. ] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ IntervalDomain(0.0,1.0),\n           x ∈ IntervalDomain(0.0,1.0)]\n# Discretization\ndx = 0.1\n\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))\n\npde_system = PDESystem(eq,bcs,domains,[t,x],[u])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# optimizer\nopt = Optim.BFGS()\nres = GalacticOptim.solve(prob,opt; cb = cb, maxiters=1200)\nphi = discretization.phi","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using Plots\n\nts,xs = [domain.domain.lower:dx:domain.domain.upper for domain in domains]\nanalytic_sol_func(t,x) =  sum([(8/(k^3*pi^3)) * sin(k*pi*x)*cos(C*k*pi*t) for k in 1:2:50000])\n\nu_predict = reshape([first(phi([t,x],res.minimizer)) for t in ts for x in xs],(length(ts),length(xs)))\nu_real = reshape([analytic_sol_func(t,x) for t in ts for x in xs], (length(ts),length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype=:contourf,title = \"analytic\");\np2 =plot(ts, xs, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(ts, xs, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: waveplot)","category":"page"},{"location":"examples/100_HJB/#Solving-a-100-dimensional-Hamilton-Jacobi-Bellman-Equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"First, here's a fully working code for the solution of a 100-dimensional Hamilton-Jacobi-Bellman equation that takes a few minutes on a laptop:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"using NeuralPDE\nusing Flux\nusing DifferentialEquations\nusing LinearAlgebra\nd = 100 # number of dimensions\nX0 = fill(0.0f0, d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nλ = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0 * sum(X.^2))\nf(X,u,σᵀ∇u,p,t) = -λ * sum(σᵀ∇u.^2)\nμ_f(X,p,t) = zero(X)  # Vector d x 1 λ\nσ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)\nhls = 10 + d # hidden layer size\nopt = Flux.ADAM(0.01)  # optimizer\n# sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d, hls, relu),\n                Dense(hls, hls, relu),\n                Dense(hls, 1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, hls, relu),\n                  Dense(hls, d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)\n@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=1.2, pabstol=1f-2)","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Now, let's explain the details!","category":"page"},{"location":"examples/100_HJB/#H-J-B-equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"H-J-B equation","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"The Hamilton-Jacobi-Bellman equation is the solution to a stochastic optimal control problem.","category":"page"},{"location":"examples/100_HJB/#Symbolic-Solution","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Symbolic Solution","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Here, we choose to solve the classical Linear Quadratic Gaussian (LQG) control problem of 100 dimensions, which is governed by the SDE dX_t = 2sqrt(λ)c_t dt + sqrt(2)dW_t where c_t is a control process. The solution to the optimal control is given by a PDE of the form:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"(Image: HJB)","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"with terminating condition g(X) = log(0.5f0 + 0.5f0*sum(X.^2)).","category":"page"},{"location":"examples/100_HJB/#Solving-LQG-Problem-with-Neural-Net","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving LQG Problem with Neural Net","text":"","category":"section"},{"location":"examples/100_HJB/#Define-the-Problem","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Define the Problem","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"To get the solution above using the TerminalPDEProblem, we write:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"d = 100 # number of dimensions\nX0 = fill(0.0f0,d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nλ = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0*sum(X.^2))\nf(X,u,σᵀ∇u,p,t) = -λ*sum(σᵀ∇u.^2)\nμ_f(X,p,t) = zero(X)  #Vector d x 1 λ\nσ_f(X,p,t) = Diagonal(sqrt(2.0f0)*ones(Float32,d)) #Matrix d x d\nprob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)","category":"page"},{"location":"examples/100_HJB/#Define-the-Solver-Algorithm","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Define the Solver Algorithm","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d dimensional -> 1 dimensional chain, while σᵀ∇u needs to be d+1 dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"hls = 10 + d #hidden layer size\nopt = Flux.ADAM(0.01)  #optimizer\n#sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = NNPDENS(u0, σᵀ∇u, opt=opt)","category":"page"},{"location":"examples/100_HJB/#Solving-with-Neural-Net","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving with Neural Net","text":"","category":"section"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\n                            alg=EM(), dt=0.2, pabstol = 1f-2)\n","category":"page"},{"location":"examples/100_HJB/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Here we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 100 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-2.","category":"page"},{"location":"pinn/ks/#Kuramoto–Sivashinsky-equation","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"Let's consider the Kuramoto–Sivashinsky equation, which contains a 4th-order derivative:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: KS)","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"with the initial and boundary conditions:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: bs)","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"with physics-informed neural networks.","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n@parameters x, t\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\nα = 1\nβ = 4\nγ = 1\neq = Dt(u(x,t)) + u(x,t)*Dx(u(x,t)) + α*Dx2(u(x,t)) + β*Dx3(u(x,t)) + γ*Dx4(u(x,t)) ~ 0\n\nu_analytic(x,t;z = -x/2+t) = 11 + 15*tanh(z) -15*tanh(z)^2 - 15*tanh(z)^3\ndu(x,t;z = -x/2+t) = 15/2*(tanh(z) + 1)*(3*tanh(z) - 1)*sech(z)^2\n\nbcs = [u(x,0) ~ u_analytic(x,0),\n       u(-10,t) ~ u_analytic(-10,t),\n       u(10,t) ~ u_analytic(10,t),\n       Dx(u(-10,t)) ~ du(-10,t),\n       Dx(u(10,t)) ~ du(10,t)]\n\n# Space and time domains\ndomains = [x ∈ IntervalDomain(-10.0,10.0),\n           t ∈ IntervalDomain(0.0,1.0)]\n# Discretization\ndx = 0.4; dt = 0.2\n\n# Neural network\nchain = FastChain(FastDense(2,12,Flux.σ),FastDense(12,12,Flux.σ),FastDense(12,1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining([dx,dt]))\npde_system = PDESystem(eq,bcs,domains,[x,t],[u])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nopt = Optim.BFGS()\nres = GalacticOptim.solve(prob,opt; cb = cb, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"And some analysis:","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using Plots\n\nxs,ts = [domain.domain.lower:dx:domain.domain.upper for (domain,dx) in zip(domains,[dx/10,dt])]\n\nu_predict = [[first(phi([x,t],res.minimizer)) for x in xs] for t in ts]\nu_real = [[u_analytic(x,t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x,t) -first(phi([x,t],res.minimizer)))  for x in xs] for t in ts]\n\np1 =plot(xs,u_predict,title = \"predict\")\np2 =plot(xs,u_real,title = \"analytic\")\np3 =plot(xs,diff_u,title = \"error\")\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: plotks)","category":"page"},{"location":"pinn/parm_estim/#Optimising-Parameters-of-a-Lorenz-System","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"","category":"section"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Consider a Lorenz System ,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"(Image: lorenzSystem)","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimise the parameters σ, β  and ρ. We start by defining the the problem,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, OrdinaryDiffEq, Plots\n@parameters t ,σ_ ,β, ρ\n@variables x(..), y(..), z(..)\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ_*(y(t) - x(t)),\n       Dt(y(t)) ~ x(t)*(ρ - z(t)) - y(t),\n       Dt(z(t)) ~ x(t)*y(t) - β*z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ IntervalDomain(0.0,1.0)]\ndt = 0.01","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"And the neural networks as,","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"input_ = length(domains)\nn = 8\nchain1 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain2 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))\nchain3 = FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1))","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"We will add an additional loss term based on the data that we have in order to optimise the parameters.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Here we simply calculate the solution of the lorenz system with OrdinaryDiffEq.jl based on the adaptivity of the ODE solver. This is used to introduce non-uniformity to the time series.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"function lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,1.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob, Tsit5(), dt=0.1)\nts = [domain.domain.lower:dt:domain.domain.upper for domain in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us,ts_]\nend\ndata = getData(sol)\ninitθs = DiffEqFlux.initial_params.([chain1,chain2,chain3])\nacum =  [0;accumulate(+, length.(initθs))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\n(u_ , t_) = data\nlen = length(data[2])","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Then we define the additional loss funciton additional_loss(phi, θ , p), the function has three arguments, phi the trial solution, θ the parameters of neural networks, and the hyperparameters p .","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"function additional_loss(phi, θ , p)\n    return sum(sum(abs2, phi[i](t_ , θ[sep[i]]) .- u_[[i], :])/len for i in 1:1:3)\nend","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"Then finally defining and optimising using the PhysicsInformedNN interface.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"discretization = NeuralPDE.PhysicsInformedNN([chain1 , chain2, chain3],NeuralPDE.GridTraining(dt), param_estim=true, additional_loss=additional_loss)\npde_system = PDESystem(eqs,bcs,domains,[t],[x, y, z],[σ_, ρ, β], defaults=Dict([p .=> 1.0 for p in [σ_, ρ, β]]))\nprob = NeuralPDE.discretize(pde_system,discretization)\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = GalacticOptim.solve(prob, BFGS(); cb = cb, maxiters=5000)\np_ = res.minimizer[end-2:end] # p_ = [9.93, 28.002, 2.667]","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"And then finally some analyisis by plotting.","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"initθ = discretization.init_params\nacum =  [0;accumulate(+, length.(initθ))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers = [res.minimizer[s] for s in sep]\nts = [domain.domain.lower:dt/10:domain.domain.upper for domain in domains][1]\nu_predict  = [[discretization.phi[i]([t],minimizers[i])[1] for t in ts] for i in 1:3]\nplot(sol)\nplot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])","category":"page"},{"location":"pinn/parm_estim/","page":"Optimising Parameters of a Lorenz System","title":"Optimising Parameters of a Lorenz System","text":"(Image: Plot_Lorenz)","category":"page"},{"location":"solvers/optimal_stopping/#Neural-Network-Solvers-for-Optimal-Stopping-Time-Problems","page":"Neural Network Solvers for Optimal Stopping Time Problems","title":"Neural Network Solvers for Optimal Stopping Time Problems","text":"","category":"section"},{"location":"solvers/optimal_stopping/","page":"Neural Network Solvers for Optimal Stopping Time Problems","title":"Neural Network Solvers for Optimal Stopping Time Problems","text":"TODO","category":"page"},{"location":"pinn/fp/#Fokker-Planck-Equation","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"","category":"section"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"Let's consider the Fokker-Planck equation:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"(Image: fke)","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"which must satisfy the normalization condition:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"(Image: nc)","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"with the boundary conditions:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"(Image: bc)","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"with Physics-Informed Neural Networks.","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n# the example is taken from this article https://arxiv.org/abs/1910.10503\n@parameters x\n@variables p(..)\nDx = Differential(x)\nDxx = Differential(x)^2\n\nα = 0.3\nβ = 0.5\n_σ = 0.5\n# Discretization\ndx = 0.05\n# here we use normalization condition: dx*p(x) ~ 1, in order to get non-zero solution.\n#(α - 3*β*x^2)*p(x) + (α*x - β*x^3)*Dx(p(x)) ~ (_σ^2/2)*Dxx(p(x))\neq  = Dx((α*x - β*x^3)*p(x)) ~ (_σ^2/2)*Dxx(p(x))+dx*p(x) - 1.\n\n# Initial and boundary conditions\nbcs = [p(-2.2) ~ 0. ,p(2.2) ~ 0. , p(-2.2) ~ p(2.2)]\n\n# Space and time domains\ndomains = [x ∈ IntervalDomain(-2.2,2.2)]\n\n# Neural network\nchain = FastChain(FastDense(1,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\n\ndiscretization = PhysicsInformedNN(chain,GridTraining(dx))\n\npde_system = PDESystem(eq,bcs,domains,[x],[p])\nprob = discretize(pde_system,discretization)\n\n#Callback function\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob,ADAM(0.1); cb = cb, maxiters=1500)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,Optim.BFGS(initial_stepnorm=0.01);allow_f_increases=true, cb = cb, maxiters=400)\n\ndx = dx/5\ndiscretization = remake(discretization; strategy = NeuralPDE.GridTraining(dx), init_params =res.minimizer)\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nres = GalacticOptim.solve(prob,ADAM(0.01); cb = cb, maxiters=1000)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,Optim.BFGS(initial_stepnorm=0.01);allow_f_increases=true, cb = cb, maxiters=400)\n\n\ndx= dx/5\ndiscretization = remake(discretization; strategy = NeuralPDE.GridTraining(dx), init_params =res.minimizer)\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nres = GalacticOptim.solve(prob,Optim.BFGS(initial_stepnorm=0.01);allow_f_increases=true, cb = cb, maxiters=400)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,ADAM(0.001); cb = cb, maxiters=1000)\n\nphi = discretization.phi","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"And some analysis:","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"using Plots\nanalytic_sol_func(x) = 28.022*exp((1/(2*_σ^2))*(2*α*x^2 - β*x^4))\n\nxs = [domain.domain.lower:dx:domain.domain.upper for domain in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.minimizer)) for x in xs]\n\nplot(xs ,u_real, label = \"analytic\")\nplot!(xs ,u_predict, label = \"predict\")","category":"page"},{"location":"pinn/fp/","page":"Fokker-Planck Equation","title":"Fokker-Planck Equation","text":"(Image: fkplot)","category":"page"},{"location":"pinn/low_level/#D-Burgers'-Equation-With-Low-Level-API","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"","category":"section"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"Let's consider the Burgers’ equation:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"(Image: burgers)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"with Physics-Informed Neural Networks. Here is an example of using the low-level API:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n@parameters t, x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n#2D PDE\neq  = Dt(u(t,x)) + u(t,x)*Dx(u(t,x)) - (0.01/pi)*Dxx(u(t,x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [u(0,x) ~ -sin(pi*x),\n       u(t,-1) ~ 0.,\n       u(t,1) ~ 0.]\n\n# Space and time domains\ndomains = [t ∈ IntervalDomain(0.0,1.0),\n           x ∈ IntervalDomain(-1.0,1.0)]\n# Discretization\ndx = 0.05\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\n\nstrategy = GridTraining(dx)\n\nphi = NeuralPDE.get_phi(chain)\nderivative = NeuralPDE.get_numeric_derivative()\ninitθ = DiffEqFlux.initial_params(chain)\n\nindvars = [t,x]\ndepvars = [u]\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq,indvars,depvars,\n                                         phi,derivative,chain,initθ,strategy)\n\nbc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars,\n                                          phi,derivative,chain,initθ,strategy,\n                                          bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n\ntrain_sets = NeuralPDE.generate_training_sets(domains,dx,[eq],bcs,indvars,depvars)\ntrain_domain_set, train_bound_set = train_sets\n\n\npde_loss_function = NeuralPDE.get_loss_function([_pde_loss_function],\n                                      train_domain_set,\n                                      strategy)\n\nbc_loss_function = NeuralPDE.get_loss_function(_bc_loss_functions,\n                                     train_bound_set,\n                                     strategy)\n\nfunction loss_function_(θ,p)\n    return pde_loss_function(θ) + bc_loss_function(θ)\nend\nf = OptimizationFunction(loss_function_, GalacticOptim.AutoZygote())\nprob = GalacticOptim.OptimizationProblem(f, initθ)\n\ncb = function (p,l)\n    println(\"Current losses are: \", pde_loss_function(p), \" , \",  bc_loss_function(p))\n    return false\nend\n\n# optimizer\nopt = ADAM(0.1)\nres = GalacticOptim.solve(prob, opt; cb = cb, maxiters=6000)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"And some analysis:","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"using Plots\n\nts,xs = [domain.domain.lower:dx:domain.domain.upper for domain in domains]\nu_predict_contourf = reshape([first(phi([t,x],res.minimizer)) for t in ts for x in xs] ,length(xs),length(ts))\nplot(ts, xs, u_predict_contourf, linetype=:contourf,title = \"predict\")\n\nu_predict = [[first(phi([t,x],res.minimizer)) for x in xs] for t in ts ]\np1= plot(xs, u_predict[2],title = \"t = 0.1\");\np2= plot(xs, u_predict[6],title = \"t = 0.5\");\np3= plot(xs, u_predict[end],title = \"t = 1\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"(Image: burgers)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"(Image: burgers2)","category":"page"},{"location":"pinn/low_level/","page":"1-D Burgers' Equation With Low-Level API","title":"1-D Burgers' Equation With Low-Level API","text":"See low-level API","category":"page"},{"location":"solvers/deep_fbsde/#Deep-Forward-Backwards-SDEs-for-Terminal-Parabolic-PDEs","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"","category":"section"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"To solve high-dimensional PDEs, one should first describe the PDE in terms of the TerminalPDEProblem with constructor:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"TerminalPDEProblem(g,f,μ_f,σ_f,X0,tspan,p=nothing)","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"which describes the semilinear parabolic PDE of the form:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"(Image: paraPDE)","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"with terminating condition u(tspan[2],x) = g(x). These methods solve the PDE in reverse, satisfying the terminal equation and giving a point estimate at u(tspan[1],X0). The dimensionality of the PDE is determined by the choice of X0, which is the initial stochastic state.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"To solve this PDE problem, there exist two algorithms:","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"NNPDENS(u0,σᵀ∇u;opt=Flux.ADAM(0.1)): Uses a neural stochastic differential equation, which is then solved by the methods available in DifferentialEquations.jl. The alg keyword is required for specifying the SDE solver algorithm that will be used on the internal SDE. All of the other keyword arguments are passed to the SDE solver.\nNNPDEHan(u0,σᵀ∇u;opt=Flux.ADAM(0.1)): Uses the stochastic RNN algorithm from Han. Only applicable when μ_f and σ_f result in a non-stiff SDE where low order non-adaptive time stepping is applicable.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"Here, u0 is a Flux.jl chain with a d-dimensional input and a 1-dimensional output. For NNPDEHan, σᵀ∇u is an array of M chains with a d-dimensional input and a d-dimensional output, where M is the total number of timesteps. For NNPDENS it is a d+1-dimensional input (where the final value is time) and a d-dimensional output. opt is a Flux.jl optimizer.","category":"page"},{"location":"solvers/deep_fbsde/","page":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"Each of these methods has a special keyword argument pabstol, which specifies an absolute tolerance on the PDE's solution, and will exit early if the loss reaches this value. Its default value is 1f-6.","category":"page"},{"location":"solvers/ode/#ODE-Specialized-Physics-Informed-Neural-Solver","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"The ODE-specialized physics-informed neural network (PINN) solver is a method for the DifferentialEquations.jl common interface of ODEProblem, which generates the solution via a neural network. Thus the standard ODEProblem is used, but a new algorithm, NNODE, is used to solve the problem.","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"The algorithm type is:","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"nnode(chain,opt)","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"where chain is a DiffEqFlux sciml_train-compatible Chain or FastChain representing a neural network, and opt is an optimization method for sciml_train. For more details, see the DiffEqFlux documentation on sciml_train.","category":"page"},{"location":"solvers/ode/","page":"ODE-Specialized Physics-Informed Neural Solver","title":"ODE-Specialized Physics-Informed Neural Solver","text":"Lagaris, Isaac E., Aristidis Likas, and Dimitrios I. Fotiadis. \"Artificial neural networks for solving ordinary and partial differential equations.\" IEEE Transactions on Neural Networks 9, no. 5 (1998): 987-1000.","category":"page"},{"location":"solvers/full_kolmogorov/#Full-Kolmogorov-PDE-Solver","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"","category":"section"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The full Kolmogorov PDE solver obtains the complete solution of a family of γ γ-parameterised Kolmogorov PDE of the form, (Image: )","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The  σ and μ are also parameterised by γ. Thus before we define the neural network solution, we figure out the dimensions required. Let's say the domain of the PDE is (Image: ) The first dimension is taken by t , and the next d dimensions are taken by x<sup>d</sup> i.e. spatial region. The next dimensions are for γ that parameterise the  σ, μ and phi (the initial condition). And thus, (Image: ) [a<sub>1</sub> | a<sub>2</sub> ....  | a<sub>n</sub>]   represents  horizontal concatenation. So for defining γ we will be defining :","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"σ_γ can be atmost d + 1 matrices with dimensions d x d  or nothing.\nμ_γ  can comprise of either ad x d matrix and a matrix of dimensions d x 1 or both, or nothing.\nphi_γ can be a matrix with dimensions k x 1.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We define the prototypes for the above parameters:","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"γ_sigma_prototype  should be a 3 dimensional matrix with first two dimensions d and d, and the last dimension should be number of matrices (at most d + 1) or it can be nothing.\nγ_mu_prototype  should comprise of 2 matrices one with dimensions d x dand other with dimensions d x 1. So we define γ_mu_prototype as (matrix1 , matrix2) we put nothing in the place if we dont require either of them. Or if we dont want it parameterised, we define,  γ_mu_prototype as nothing\nγ_phi_prototype should be a matrix with k x 1 dimensions.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"Now lets define a test problem,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We start by defining the dimension d of the solution and the prototypes for the problem.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"d = 1\nγ_mu_prototype = nothing\nγ_sigma_prototype = zeros(d , d , 1)\nγ_phi_prototype = nothing","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And now the neural network solution with the number of dimensions we require at input.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"m = Chain(Dense(3 , 16, elu),Dense(16, 16, elu) , Dense(16 , 5 , elu) , Dense(5 , 1))","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we write the domains of t , x, γ_sigma, γ_mu and γ_phi. And thus we use KolmogorovParamDomain to pass the domains of   γ_sigma, γ_mu and γ_phi as arguments, respectively.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"tspan = (0.00 , 1.00)\nxspan = (0.00 , 3.00)\ny_domain = KolmogorovParamDomain((0.00 , 2.00) , (0.00 , 0.00) , (0.00 , 0.00) )","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"The functions σ(x , γ_sigma) where  and μ(x , γ_mu1, γ_mu2) (γ_mu1 is the d x d x 1 matrix and γ_mu2 is a d x 1 x 1 matrix)","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"g(x , γ_sigma) = γ_sigma[: , : , 1] #the σ(x , γ_sigma)\nf(x , γ_mu_1 ,γ_mu_2 ) = [0.00] # the μ(x , γ_mu1, γ_mu2)","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we define the function phi(x , γ_phi) where γ_phi is a matrix with k x 1 x trajectories . Thus the function should return an array with dimensions 1 x trajectories.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"function phi(x , y_phi)\n  x.^2\nend","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"We define the dt , dx and dy for the equation,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"dt = 0.01\ndx = 0.01\ndy = 0.01","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"And then we finally define the optimiser, the algorithms for SDE solution and ensemble simulation.","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"opt = Flux.ADAM(1e-2)\nsdealg = EM()\nensemblealg = EnsembleThreads()","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"Finally we define the ParamKolmogorovPDEProblem ,","category":"page"},{"location":"solvers/full_kolmogorov/","page":"Full Kolmogorov PDE Solver","title":"Full Kolmogorov PDE Solver","text":"prob = ParamKolmogorovPDEProblem(f , g , phi , xspan , tspan , d  , y_domain  ; Y_sigma_prototype = γ_sigma_prototype)\nsol = solve(prob, NNParamKolmogorov(m,opt , sdealg,ensemblealg) , verbose = true, dt = 0.01,\n            abstol=1e-10, dx = 0.01 , trajectories = trajectories ,  maxiters = 150 , use_gpu = false)","category":"page"},{"location":"examples/optimal_stopping_american/#Optimal-Stopping-Times-of-American-Options","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"","category":"section"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Here, we will aim to solve an optimal stopping problem using the NNStopping algorithm.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Let us consider standard American options. Unlike European options, American options can be exercized before their maturity and thus the problem reduces to finding an optimal stopping time.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"As stated above, since we can execute the option at any optimal time before the maturity of the option, the standard Black-Scholes model gets modified to:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"  fracVt + rSfracVS + frac12σ^2S^2frac^2 VS^2 -rV  0","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The stock price will follow a standard geometric brownian motion given by:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"  dS_t = rS_tdt + σS_tdW_t","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"And thus our final aim will be to calculate: <img src=\"https://raw.githubusercontent.com/ashutosh-b-b/github-doc-images/master/Price%20of%20American%20Option.png\">","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We will be using a SDEProblem to denote a problem of this type. We can define this as a SDEProblem and add a terminal condition g in order to price the American Options.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We will take the case of an American max put option with strike price K, constant volatility β, a risk-free rate r, the initial stock price u0 = 80.00, the maturity T, and number of steps N. The forcing function f and noise function sigma are defined for the type of model. See StochasticDiffEq documentation.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"d = 1 #Dimensions of initial stock price\nr = 0.04f0\nbeta = 0.2f0\nK = 100.00\nT = 1.0\nu0 = fill(80.00 , d , 1) #Initial Stock Price\n#Defining the drift (f) and diffusion(sigma)\nf(du,u,p,t) = (du .= r*u)\nsigma(du,u,p,t)  = (du .= Diagonal(beta*u))\n\ntspan = (0.0 , T)\nN = 50\ndt = tspan[2]/(N - 1)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The final part is the payoff function:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"<img src=\"https://raw.githubusercontent.com/ashutosh-b-b/github-doc-images/master/payoff_function.png\">","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"The discounted payoff function is:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"function g(t , x)\n  return exp(-r*t)*(max(K -  maximum(x)  , 0))\nend","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Now, in order to define an optimal stopping problem, we will use the SDEProblem and pass the discounted payoff function g as an kwarg.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"prob  = SDEProblem(f , sigma , u0 , tspan ; g = g)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Finally, let's build our neural network model using Flux.jl. Note that the final layer should be the softmax (Flux.softmax) function as we need the sum of probabilities at all stopping times to be 1. And then add an optimizer function.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"m = Chain(Dense(d , 5, tanh), Dense(5, 16 , tanh)  , Dense(16 , N ), softmax)\nopt = Flux.ADAM(0.1)","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"We add algorithms to solve the SDE and the Ensemble. These are the algorithms required to solve the SDEProblem (we use the Euler-Maruyama algorithm in this case) and the EnsembleProblem to run multiple simulations. See Ensemble Algorithms.","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"sdealg = EM()\nensemblealg = EnsembleThreads()","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"Finally, we call the solve function:","category":"page"},{"location":"examples/optimal_stopping_american/","page":"Optimal Stopping Times of American Options","title":"Optimal Stopping Times of American Options","text":"sol = solve(prob, NeuralPDE.NNStopping( m, opt , sdealg , ensemblealg), verbose = true, dt = dt,\n            abstol=1e-6, maxiters = 20 , trajectories = 200)\n","category":"page"},{"location":"pinn/2D/#dimensional-PDEs-with-GPU","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"","category":"section"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"the 2-dimensional PDE: (Image: 3dpde)","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"with the initial and boundary conditions:","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: boundary)","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"on the space and time domain:","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: space)","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"with physics-informed neural networks.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\nusing Quadrature, Cuba, CUDA\n# 3D PDE\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min= 0.\nt_max = 2.0\nx_min = 0.\nx_max = 2.\ny_min = 0.\ny_max = 2.\n\n# 3D PDE\neq  = Dt(u(t,x,y)) ~ Dxx(u(t,x,y)) + Dyy(u(t,x,y))\n\nanalytic_sol_func(t,x,y) = exp(x+y)*cos(x+y+4t)\n# Initial and boundary conditions\nbcs = [u(t_min,x,y) ~ analytic_sol_func(t_min,x,y),\n       u(t,x_min,y) ~ analytic_sol_func(t,x_min,y),\n       u(t,x_max,y) ~ analytic_sol_func(t,x_max,y),\n       u(t,x,y_min) ~ analytic_sol_func(t,x,y_min),\n       u(t,x,y_max) ~ analytic_sol_func(t,x,y_max)]\n\n# Space and time domains\ndomains = [t ∈ IntervalDomain(t_min,t_max),\n           x ∈ IntervalDomain(x_min,x_max),\n           y ∈ IntervalDomain(y_min,y_max)]\n\n# Neural network\ninner = 10\nchain = FastChain(FastDense(3,inner,Flux.σ),\n                  FastDense(inner,inner,Flux.σ),\n                  FastDense(inner,inner,Flux.σ),\n                  FastDense(inner,1)))\n\ninitθ = DiffEqFlux.initial_params(chain) |> gpu\n\nstrategy = NeuralPDE.QuasiRandomTraining(6000; #points\n                                         sampling_alg = UniformSample(),\n                                         minibatch = 100)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,\n                                             strategy;\n                                             init_params = initθ)\n\npde_system = PDESystem(eq,bcs,domains,[t,x,y],[u])\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nres = GalacticOptim.solve(prob,ADAM(0.1);cb=cb,maxiters=1000)","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"The remake function allows to rebuild the PDE problem.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,ADAM(0.01);cb=cb,maxiters=1000)\nprob = remake(prob,u0=res.minimizer)\nres = GalacticOptim.solve(prob,ADAM(0.001);cb=cb,maxiters=1000)\n\nphi = discretization.phi\nts,xs,ys = [domain.domain.lower:0.1:domain.domain.upper for domain in domains]\nu_real = [analytic_sol_func(t,x,y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi([t, x, y], res.minimizer))) for t in ts for x in xs for y in ys]\n\n@test u_predict ≈ u_real atol = 20.0\n\nusing Plots\nusing Printf\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t,x,y) for x in xs for y in ys], (length(xs),length(ys)))\n        u_predict = reshape([Array(phi([t, x, y], res.minimizer))[1] for x in xs for y in ys], length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st=:contourf,label=\"\", title=title)\n        plot(p1,p2,p3)\n    end\n    gif(anim,\"3pde.gif\", fps=10)\nend\n\nplot_(res)\n","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: 3pde)","category":"page"},{"location":"pinn/2D/#Performance-benchmarks","page":"2-dimensional PDEs with GPU","title":"Performance benchmarks","text":"","category":"section"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"Here are some performance benchmarks for 3pde with various number of input points and the number of neurons in the hidden layer, measuring the time for 100 iterations. Сomparing runtime with GPU and CPU.","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"\njulia> CUDA.device()\nCuDevice(0): Tesla P100-PCIE-16GB\n","category":"page"},{"location":"pinn/2D/","page":"2-dimensional PDEs with GPU","title":"2-dimensional PDEs with GPU","text":"(Image: image)","category":"page"},{"location":"pinn/3rd/#ODE-with-a-3rd-Order-Derivative","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"","category":"section"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"Let's consider the ODE with a 3rd-order derivative:","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"(Image: hdode)","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We will use physics-informed neural networks.","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n@parameters x\n@variables u(..)\n\nDxxx = Differential(x)^3\nDx = Differential(x)\n# ODE\neq = Dxxx(u(x)) ~ cos(pi*x)\n\n# Initial and boundary conditions\nbcs = [u(0.) ~ 0.0,\n       u(1.) ~ cos(pi),\n       Dx(u(1.)) ~ 1.0]\n\n# Space and time domains\ndomains = [x ∈ IntervalDomain(0.0,1.0)]\n\n# Neural network\nchain = FastChain(FastDense(1,8,Flux.σ),FastDense(8,1))\n\ndiscretization = PhysicsInformedNN(chain, QuasiRandomTraining(20))\npde_system = PDESystem(eq,bcs,domains,[x],[u])\nprob = discretize(pde_system,discretization)\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, ADAM(0.01); cb = cb, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We can plot the predicted solution of the ODE and its analytical solution.","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using Plots\n\nanalytic_sol_func(x) = (π*x*(-x+(π^2)*(2*x-3)+1)-sin(π*x))/(π^3)\n\ndx = 0.05\nxs = [domain.domain.lower:dx/10:domain.domain.upper for domain in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.minimizer)) for x in xs]\n\nx_plot = collect(xs)\nplot(x_plot ,u_real,title = \"real\")\nplot!(x_plot ,u_predict,title = \"predict\")","category":"page"},{"location":"pinn/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"(Image: hodeplot)","category":"page"},{"location":"pinn/poisson/#Poisson-Equation","page":"Poisson Equation","title":"Poisson Equation","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"In this example, we will solve a Poisson equation:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"(Image: poisson)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"with the boundary conditions:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"(Image: boundary)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"on the space domain:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"(Image: spaces)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"with grid discretization dx = 0.1. We will use physics-informed neural networks.","category":"page"},{"location":"pinn/poisson/#Copy-Pastable-Code","page":"Poisson Equation","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.f0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.f0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ IntervalDomain(0.0,1.0),\n           y ∈ IntervalDomain(0.0,1.0)]\n\n# Neural network\ndim = 2 # number of dimensions\nchain = FastChain(FastDense(dim,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\n\n# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain,GridTraining(dx))\n\npde_system = PDESystem(eq,bcs,domains,[x,y],[u])\nprob = discretize(pde_system,discretization)\n\n#Optimizer\nopt = Optim.BFGS()\n\n#Callback function\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, opt, cb = cb, maxiters=1000)\nphi = discretization.phi\n\nusing Plots\n\nxs,ys = [domain.domain.lower:dx/10:domain.domain.upper for domain in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/poisson/#Detailed-Description","page":"Poisson Equation","title":"Detailed Description","text":"","category":"section"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"The ModelingToolkit PDE interface for this example looks like this:","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux\n@parameters x y\n@variables u(..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.f0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.f0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ IntervalDomain(0.0,1.0),\n           y ∈ IntervalDomain(0.0,1.0)]","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we define the neural network, where the input of NN equals the number of dimensions and output equals the number of equations in the system.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"# Neural network\ndim = 2 # number of dimensions\nchain = FastChain(FastDense(dim,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we build PhysicsInformedNN algorithm where dx is the step of discretization and strategy stores information for choosing a training strategy.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"As described in the API docs, we now need to define the PDESystem and create PINNs problem using the discretize method.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"pde_system = PDESystem(eq,bcs,domains,[x,y],[u])\nprob = discretize(pde_system,discretization)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"Here, we define the callback function and the optimizer. And now we can solve the PDE using PINNs (with the number of epochs maxiters=1000).","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"#Optimizer\nopt = Optim.BFGS()\n\ncb = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = GalacticOptim.solve(prob, opt, cb = cb, maxiters=1000)\nphi = discretization.phi","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"xs,ys = [domain.domain.lower:dx/10:domain.domain.upper for domain in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"pinn/poisson/","page":"Poisson Equation","title":"Poisson Equation","text":"(Image: poissonplot)","category":"page"},{"location":"examples/ode/#Solving-ODEs-with-Neural-Networks","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"","category":"section"},{"location":"examples/ode/","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"The following is an example of solving a DifferentialEquations.jl ODEProblem with a neural network using the physics-informed neural networks approach specialized to 1-dimensional PDEs (ODEs).","category":"page"},{"location":"examples/ode/","page":"Solving ODEs with Neural Networks","title":"Solving ODEs with Neural Networks","text":"using Flux, Optim\nusing NeuralPDE\n# Run a solve on scalars\nlinear = (u, p, t) -> cos(2pi * t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0, tspan)\nchain = Flux.Chain(Dense(1, 5, σ), Dense(5, 1))\nopt = Flux.ADAM(0.1, (0.9, 0.95))\n@time sol = solve(prob, NeuralPDE.NNODE(chain, opt), dt=1 / 20f0, verbose=true,\n            abstol=1e-10, maxiters=200)","category":"page"},{"location":"examples/kolmogorovbackwards/#Solving-Kolmogorov-Equations-with-Neural-Networks","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"","category":"section"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"A Kolmogorov PDE is of the form :","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: KPDE)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Consider S to be a solution process to the SDE:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: StochasticP)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"then the solution to the Kolmogorov PDE is given as:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"(Image: Solution)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"A Kolmogorov PDE Problem can be defined using a SDEProblem:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"SDEProblem(μ,σ,u0,tspan,xspan,d)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, u0 is the initial distribution of x. Here, we define u(0,x) as the probability density function of u0.μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x. u0 can be defined using Distributions.jl.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Another way of defining a KolmogorovPDE is to use the KolmogorovPDEProblem.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"KolmogorovPDEProblem(μ,σ,phi,tspan,xspan,d)","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, phi is the initial condition on u(t,x) when t = 0. μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"To solve this problem use:","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"NNKolmogorov(chain, opt , sdealg): Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation.","category":"page"},{"location":"examples/kolmogorovbackwards/","page":"Solving Kolmogorov Equations with Neural Networks","title":"Solving Kolmogorov Equations with Neural Networks","text":"Here, chain is a Flux.jl chain with a d-dimensional input and a 1-dimensional output.opt is a Flux.jl optimizer. And sdealg is a high-order algorithm to calculate the solution for the SDE, which is used to define the learning data for the problem. Its default value is the classic Euler-Maruyama algorithm.","category":"page"},{"location":"solvers/pinns/#Physics-Informed-Neural-Networks","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Using the PINNs solver, we can solve general nonlinear PDEs:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"(Image: generalPDE)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"with suitable boundary conditions:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"(Image: bcs)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"where time t is a special component of x, and Ω contains the temporal domain.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"PDEs are defined using the ModelingToolkit.jl PDESystem:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"pde_system = PDESystem(eq,bcs,domains,param,var)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Here, eq is the equation, bcs represents the boundary conditions, param is the parameter of the equation (like [x,y]), and var represents variables (like [u]).","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"The PhysicsInformedNN discretizer is defined as:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"discretization = PhysicsInformedNN(chain,\n                                   strategy;\n                                   init_params = nothing,\n                                   phi = nothing,\n                                   derivative = nothing,\n                                   )","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"Keyword arguments:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"chain is a Flux.jl chain, where the input of NN equals the number of dimensions and output equals the number of equations in the system,\nstrategy determines which training strategy will be used,\ninit_params is the initial parameter of the neural network. If nothing then automatically generated from the neural network,\nphi is a trial solution,\nderivative is a method that calculates the derivative.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"The method discretize interprets from the ModelingToolkit PDE form to the PINNs Problem.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"prob = discretize(pde_system, discretization)","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"which outputs an OptimizationProblem for GalacticOptim.jl.","category":"page"},{"location":"solvers/pinns/#Training-strategy","page":"Physics-Informed Neural Networks","title":"Training strategy","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"List of training strategies that are available now:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"GridTraining(dx): Initialize points on a lattice uniformly spaced via dx. If dx is a scalar, then dx corresponds to the spacing in each direction. If dx is a vector, then it should be sized to match the number of dimensions and corresponds to the spacing per direction.\nStochasticTraining(points): points number of sochastically sampled points from the domain. In each optimization iteration, we randomly select a new subset of points from a full training set.\nQuasiRandomTraining(points;sampling_alg = UniformSample(),minibatch=500): The training set is generated on quasi-random low discrepency sequences. minibatch is the number of subsets, where points is the number of quasi-random points in minibatch. The number of the total points is length(lb) * points * minibatch,","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"where lb is the lower bound and length(lb) is the dimensionality.   sampling_alg is the quasi-Monte Carlo sampling algorithm.   On each iteration of training, it is randomly selected one of the minibatch.   See the QuasiMonteCarlo.jl for   the full set of quasi-random sampling algorithms which are available.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"QuadratureTraining(;quadrature_alg=CubatureJLh(),reltol= 1e-6,abstol= 1e-3,maxiters=1e3,batch=100): The loss is computed as an approximation of the integral of the PDE loss at each iteration using adaptive quadrature methods via the differentiable Quadrature.jl.\nquadrature_alg is quadrature algorithm,\nreltol: relative tolerance,\nabstol: absolute tolerance,\nmaxiters: the maximum number of iterations in quadrature algorithm,\nbatch: the preferred number of points to batch. If batch = 0, the number of points in the batch is determined adaptively by the algorithm.\nSee the Quadrature.jl documentation for the choices of quadrature methods.","category":"page"},{"location":"solvers/pinns/#Low-level-API","page":"Physics-Informed Neural Networks","title":"Low-level API","text":"","category":"section"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"These additional methods exist to help with introspection:","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"symbolic_discretize(pde_system,discretization): This method is the same as discretize but instead returns the unevaluated Julia function to allow the user to see the generated training code.\nbuild_symbolic_loss_function(eqs,indvars,depvars, phi, derivative, initθ; bc_indvars=nothing): return symbolic inner representation for the loss function.   Keyword arguments:\neqs: equation or equations,\nindvars: independent variables (the parameter of the equation),\ndepvars: dependent variables,\nphi:trial solution,\nderivative: method that calculates the derivative,\ninitθ: the initial parameter of the neural network,\nbc_indvars: independent variables for each boundary conditions.\nbuild_symbolic_equation(eq,indvars,depvars): return symbolic inner representation for the equation.\nbuild_loss_function(eqs, indvars, depvars, phi, derivative, initθ; bc_indvars=nothing): returns the body of loss function, which is the executable Julia function, for the main equation or boundary condition.\nget_loss_function(loss_functions, train_sets, strategy::TrainingStrategies; τ = nothing): return the executable loss function.  Keyword arguments:\nloss_functions: the body of loss function, which is created using  build_loss_function,\ntrain_sets: training sets,\nstrategy: training strategy,\nτ: normalizing coefficient for loss function. If τ is nothing, then it is automatically set to 1/n where n is the number of points checked in the loss function.\nget_phi(chain): return function for trial solution.\nget_numeric_derivative(): return method that calculates the derivative.\ngenerate_training_sets(domains,dx,bcs,_indvars::Array,_depvars::Array): return training sets for equations and boundary condition, that is used for GridTraining strategy.\nget_variables(eqs,_indvars::Array,_depvars::Array): returns all variables that are used in each equations or boundary condition.\nget_argument(eqs,_indvars::Array,_depvars::Array): returns all arguments that are used in each equations or boundary condition.\nget_bounds(domains,bcs,_indvars::Array,_depvars::Array): return pairs with lower and upper bounds for all domains. It is used for all non-grid training strategy: StochasticTraining, QuasiRandomTraining, QuadratureTraining.","category":"page"},{"location":"solvers/pinns/","page":"Physics-Informed Neural Networks","title":"Physics-Informed Neural Networks","text":"See how this can be used in Debugging section or 2-D Burgers equation, low-level API  examples.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/#Neural-Network-Solvers-for-Kolmogorov-Backwards-Equations","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"","category":"section"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"A Kolmogorov PDE is of the form :","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Considering S to be a solution process to the SDE:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"then the solution to the Kolmogorov PDE is given as:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"(Image: )","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"A Kolmogorov PDE Problem can be defined using a SDEProblem:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"SDEProblem(μ,σ,u0,tspan,xspan,d)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, u0 is the initial distribution of x. Here, we define u(0,x) as the probability density function of u0.μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x. u0 can be defined using Distributions.jl.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Another way of defining a KolmogorovPDE is to use the KolmogorovPDEProblem.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"KolmogorovPDEProblem(μ,σ,phi,tspan,xspan,d)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, phi is the initial condition on u(t,x) when t = 0. μ and σ are obtained from the SDE for the stochastic process above. d represents the dimensions of x.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"To solve this problem use:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"NNKolmogorov(chain, opt , sdealg): Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Here, chain is a Flux.jl chain with a d-dimensional input and a 1-dimensional output.opt is a Flux.jl optimizer. And sdealg is a high-order algorithm to calculate the solution for the SDE, which is used to define the learning data for the problem. Its default value is the classic Euler-Maruyama algorithm.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/#Using-GPU-for-Kolmogorov-Equations","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Using GPU for Kolmogorov Equations","text":"","category":"section"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"For running Kolmogorov Equations on a GPU, there are certain aspects that are need to be taken care of:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Convert the model parameters to CuArrays using the fmap function given by Flux.jl:","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"m = Chain(Dense(1, 64, σ), Dense(64, 64, σ) , Dense(5, 2))\nm = fmap(cu, m)","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"Unlike other solvers, we need to specify explicitly that the solver is to run on the GPU. This can be done by passing the use_gpu = true into the solver.","category":"page"},{"location":"solvers/kolmogorovbackwards_solver/","page":"Neural Network Solvers for Kolmogorov Backwards Equations","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"solve(prob, NeuralPDE.NNKolmogorov(m, opt, sdealg, ensemblealg), use_gpu = true,  verbose = true, dt = dt, dx = dx , trajectories = trajectories , abstol=1e-6, maxiters = 1000)","category":"page"},{"location":"#NeuralPDE.jl:-Scientific-Machine-Learning-for-Partial-Differential-Equations","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning for Partial Differential Equations","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"NeuralPDE.jl is a solver package which consists of neural network solvers for partial differential equations using scientific machine learning (SciML) techniques such as physics-informed neural networks (PINNs) and deep BSDE solvers. This package utilizes deep neural networks and neural stochastic differential equations to solve high-dimensional PDEs at a greatly reduced cost and greatly increased generality compared with classical methods.","category":"page"},{"location":"#Features","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"Features","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"Physics-Informed Neural Networks for automated PDE solving\nForward-Backwards Stochastic Differential Equation (FBSDE) methods for parabolic PDEs\nDeep-learning-based solvers for optimal stopping time and Kolmogorov backwards equations","category":"page"},{"location":"#Citation","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"Citation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"If you use NeuralPDE.jl in your work, please cite:","category":"page"},{"location":"","page":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","title":"NeuralPDE.jl: Scientific Machine Learning (SciML) for Partial Differential Equations","text":"@article{DifferentialEquations.jl-2017,\n author = {Rackauckas, Christopher and Nie, Qing},\n doi = {10.5334/jors.151},\n journal = {The Journal of Open Research Software},\n keywords = {Applied Mathematics},\n note = {Exported from https://app.dimensions.ai on 2019/05/05},\n number = {1},\n pages = {},\n title = {DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia},\n url = {https://app.dimensions.ai/details/publication/pub.1085583166 and http://openresearchsoftware.metajnl.com/articles/10.5334/jors.151/galley/245/download/},\n volume = {5},\n year = {2017}\n}","category":"page"}]
}
